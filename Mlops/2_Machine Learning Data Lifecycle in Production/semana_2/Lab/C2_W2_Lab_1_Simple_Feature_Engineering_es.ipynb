{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPt5BHTwy_0F"
   },
   "source": [
    "# Ungraded Lab: Ingeniería de características simple\n",
    "\n",
    "En este laboratorio, obtendrás algo de práctica con la biblioteca [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started) (o `tf.Transform`). Esto sirve para mostrar lo que sucede bajo el capó cuando se llega a utilizar el componente [TFX Transform](https://www.tensorflow.org/tfx/guide/transform) dentro de una tubería TFX en los próximos laboratorios. Los fragmentos de código y las discusiones principales están tomados de este [TensorFlow official notebook](https://www.tensorflow.org/tfx/tutorials/transform/simple) pero hemos expuesto algunos puntos clave.\n",
    "\n",
    "El preprocesamiento es a menudo necesario en los proyectos de ML porque los datos en bruto aún no están en un formato adecuado para el entrenamiento de un modelo. Si no se hace así, el modelo no converge o tiene un rendimiento deficiente. Algunas transformaciones estándar incluyen la normalización de los valores de los píxeles, la bucketización, la codificación de un solo disparo, y similares. Por lo tanto, estas mismas transformaciones deben hacerse también durante la inferencia para asegurar que el modelo está calculando las predicciones correctas.\n",
    "\n",
    "Con Tensorflow Transform, puede preprocesar los datos utilizando el mismo código tanto para entrenar un modelo como para servir a las inferencias en producción. Proporciona varias funciones de utilidad para las tareas comunes de preprocesamiento, incluyendo la creación de características que requieren una pasada completa sobre el conjunto de datos de entrenamiento. Los resultados son las características transformadas y un gráfico TensorFlow que se puede utilizar tanto para el entrenamiento como para el servicio. El uso del mismo gráfico para el entrenamiento y el servicio puede evitar la distorsión de las características, ya que se aplican las mismas transformaciones en ambas etapas.\n",
    "\n",
    "Para este ejercicio introductorio, recorrerás el \"Hola Mundo\" del uso de TensorFlow Transform para preprocesar los datos de entrada. Como has visto en clase, los pasos principales son:\n",
    "\n",
    "1. Recoger los datos en bruto\n",
    "2. Definir los metadatos\n",
    "3. Crear una función de preprocesamiento\n",
    "4. Generar un gráfico constante con las transformaciones necesarias\n",
    "\n",
    "¡Comencemos!\n",
    "\n",
    "\n",
    "<details><summary>Texto Original</summary>\n",
    "\n",
    "\n",
    "# Ungraded Lab: Simple Feature Engineering\n",
    "\n",
    "In this lab, you will get some hands-on practice with the [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started) library (or `tf.Transform`). This serves to show what is going on under the hood when you get to use the [TFX Transform](https://www.tensorflow.org/tfx/guide/transform) component within a TFX pipeline in the next labs. The code snippets and main discussions are taken from this [TensorFlow official notebook](https://www.tensorflow.org/tfx/tutorials/transform/simple) but we have expounded on a few key points.\n",
    "\n",
    "Preprocessing is often required in ML projects because the raw data is not yet in a suitable format for training a model. Not doing so usually results in the model not converging or having poor performance. Some standard transformations include normalizing pixel values, bucketizing, one-hot encoding, and the like. Consequently, these same transformations should also be done during inference to ensure that the model is computing the correct predictions.\n",
    "\n",
    "With Tensorflow Transform, you can preprocess data using the same code for both training a model and serving inferences in production. It provides several utility functions for common preprocessing tasks including creating features that require a full pass over the training dataset. The outputs are the transformed features and a TensorFlow graph which you can use for both training and serving. Using the same graph for both training and serving can prevent feature skew, since the same transformations are applied in both stages.\n",
    "\n",
    "For this introductory exercise, you will walk through the \"Hello World\" of using TensorFlow Transform to preprocess input data. As you've seen in class, the main steps are to:\n",
    "\n",
    "1. Collect raw data\n",
    "2. Define metadata\n",
    "3. Create a preprocessing function\n",
    "4. Generate a constant graph with the required transformations\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTaw5L5AdODU"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K4QXVIM7iglN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.6.0\n",
      "TFX Transform version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'TFX Transform version: {tft.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxOxaaOYRfl7"
   },
   "source": [
    "## Recoger los datos en bruto\n",
    "\n",
    "En primer lugar, tendrá que cargar sus datos. Para simplificar, no utilizaremos un conjunto de datos real en este ejercicio. Lo harás en el próximo laboratorio. Por ahora, sólo usarás datos ficticios para que puedas inspeccionar las transformaciones más fácilmente.\n",
    "\n",
    "<details><summary>Texto Original</summary>\n",
    "\n",
    "\n",
    "## Collect raw data\n",
    "\n",
    "First, you will need to load your data. For simplicity, we will not use a real dataset in this exercise. You will do that in the next lab. For now, you will just use dummy data so you can inspect the transformations more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-R236Tkf_ON3"
   },
   "outputs": [],
   "source": [
    "# define sample data\n",
    "raw_data = [\n",
    "      {'x': 1, 'y': 1, 's': 'hello'},\n",
    "      {'x': 2, 'y': 2, 's': 'world'},\n",
    "      {'x': 3, 'y': 3, 's': 'hello'}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILuo5t4PdODW"
   },
   "source": [
    "## Definir los metadatos\n",
    "\n",
    "A continuación, definiremos los metadatos. Esto contiene el esquema que indica los tipos de cada columna de características (o clave) en `raw_data`. Debes tener en cuenta algunas cosas:\n",
    "\n",
    "* La función de transformación espera que los metadatos estén empaquetados en un objeto [DatasetMetadata](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/dataset_metadata.py#L23). \n",
    "* El constructor de la clase `DatasetMetadata` espera un tipo de datos [Schema protocol buffer](https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/schema.proto#L46). Puede utilizar el método [schema_from_feature_spec()](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/schema_utils.py#L36) para generarlo a partir de un diccionario.\n",
    "* Para construir dicho diccionario, utilizará las claves/nombres de columnas de `raw_data` y asignará un [FeatureSpecType](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/common_types.py#L29) como valores. Esto permite especificar si la entrada es de longitud fija o variable (utilizando las clases [tf.io](https://www.tensorflow.org/api_docs/python/tf/io)), así como definir la forma y el tipo de datos.\n",
    "\n",
    "Vea cómo se implementa esto en la celda de abajo.\n",
    "\n",
    "<details><summary>Texto Original</summary>\n",
    "\n",
    "\n",
    "## Define the metadata\n",
    "\n",
    "Next, you will define the metadata. This contains the schema that tells the types of each feature column (or key) in `raw_data`. You need to take note of a few things:\n",
    "\n",
    "* The transform function later expects the metadata to be packed in a [DatasetMetadata](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/dataset_metadata.py#L23) object. \n",
    "* The constructor for the `DatasetMetadata` class expects a [Schema protocol buffer](https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/schema.proto#L46) data type. You can use the [schema_from_feature_spec()](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/schema_utils.py#L36) method to generate that from a dictionary.\n",
    "* To build the said dictionary, you will use the keys/column names of `raw_data` and assign a [FeatureSpecType](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/common_types.py#L29) as values. This allows you to specify if the input is fixed or variable length (using [tf.io](https://www.tensorflow.org/api_docs/python/tf/io) classes), as well as to define the shape and data type.\n",
    "\n",
    "See how this is implemented in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eqs_uVAtdODW"
   },
   "outputs": [],
   "source": [
    "# define the schema as a DatasetMetadata object\n",
    "raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
    "    \n",
    "    # use convenience function to build a Schema protobuf\n",
    "    schema_utils.schema_from_feature_spec({\n",
    "        \n",
    "        # define a dictionary mapping the keys to its feature spec type\n",
    "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
    "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
    "        's': tf.io.FixedLenFeature([], tf.string),\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xZ6cdMhGdODW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"s\"\n",
      "  type: BYTES\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "  }\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"x\"\n",
      "  type: FLOAT\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "  }\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"y\"\n",
      "  type: FLOAT\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "  }\n",
      "  shape {\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preview the schema\n",
    "print(raw_data_metadata._schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zadh6MXLS3eD"
   },
   "source": [
    "## Create a preprocessing function\n",
    "The _preprocessing function_ is the most important concept of `tf.Transform`. A preprocessing function is where the transformation of the dataset really happens. It accepts and returns a dictionary of tensors, where a tensor means a <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Tensor'><code>Tensor</code></a> or <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/SparseTensor'><code>SparseTensor</code></a>. There are two main groups of API calls that typically form the heart of a preprocessing function:\n",
    "\n",
    "1. **TensorFlow Ops:** Any function that accepts and returns tensors. These add TensorFlow operations to the graph that transforms raw data into transformed data one feature vector at a time.  These will run for every example, during both training and serving.\n",
    "2. **TensorFlow Transform Analyzers:** Any of the analyzers provided by `tf.Transform`. Analyzers also accept and return tensors, but unlike TensorFlow ops they only run once during training, and typically make a full pass over the entire training dataset. They create <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/constant'>tensor constants</a>, which are added to your graph. For example, `tft.min` computes the minimum of a tensor over the training dataset.\n",
    "\n",
    "*Precaución: Cuando aplica su función de preprocesamiento para servir inferencias, las constantes que fueron creadas por los analizadores durante el entrenamiento no cambian.  Si sus datos tienen componentes de tendencia o estacionalidad, planifique en consecuencia.*\n",
    "\n",
    "Puede ver las funciones disponibles para transformar sus datos [aquí](https://www.tensorflow.org/tfx/transform/api_docs/python/tft).\n",
    "\n",
    "\n",
    "<details><summary>Texto Original</summary>\n",
    "\n",
    "## Create a preprocessing function\n",
    "The _preprocessing function_ is the most important concept of `tf.Transform`. A preprocessing function is where the transformation of the dataset really happens. It accepts and returns a dictionary of tensors, where a tensor means a <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Tensor'><code>Tensor</code></a> or <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/SparseTensor'><code>SparseTensor</code></a>. There are two main groups of API calls that typically form the heart of a preprocessing function:\n",
    "\n",
    "1. **TensorFlow Ops:** Any function that accepts and returns tensors. These add TensorFlow operations to the graph that transforms raw data into transformed data one feature vector at a time.  These will run for every example, during both training and serving.\n",
    "2. **TensorFlow Transform Analyzers:** Any of the analyzers provided by `tf.Transform`. Analyzers also accept and return tensors, but unlike TensorFlow ops they only run once during training, and typically make a full pass over the entire training dataset. They create <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/constant'>tensor constants</a>, which are added to your graph. For example, `tft.min` computes the minimum of a tensor over the training dataset.\n",
    "\n",
    "*Caution: When you apply your preprocessing function to serving inferences, the constants that were created by analyzers during training do not change.  If your data has trend or seasonality components, plan accordingly.*\n",
    "\n",
    "You can see available functions to transform your data [here](https://www.tensorflow.org/tfx/transform/api_docs/python/tft)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "H2wANNF_2dCR"
   },
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "    \n",
    "    # extract the columns and assign to local variables\n",
    "    x = inputs['x']\n",
    "    y = inputs['y']\n",
    "    s = inputs['s']\n",
    "    \n",
    "    # data transformations using tft functions\n",
    "    x_centered = x - tft.mean(x)\n",
    "    y_normalized = tft.scale_to_0_1(y)\n",
    "    s_integerized = tft.compute_and_apply_vocabulary(s)\n",
    "    x_centered_times_y_normalized = (x_centered * y_normalized)\n",
    "    \n",
    "    # return the transformed data\n",
    "    return {\n",
    "        'x_centered': x_centered,\n",
    "        'y_normalized': y_normalized,\n",
    "        's_integerized': s_integerized,\n",
    "        'x_centered_times_y_normalized': x_centered_times_y_normalized,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSl9qyTCbBKR"
   },
   "source": [
    "## Generar un gráfico constante con las transformaciones requeridas\n",
    "\n",
    "Ahora estás listo para poner todo junto y transformar tus datos. Al igual que TFDV la semana pasada, Tensorflow Transform también utiliza [Apache Beam](https://beam.apache.org/) para la escalabilidad y flexibilidad del despliegue. Como verás a continuación, Beam utiliza el operador de tubería (`|`) para apilar las diferentes etapas de la tubería. En este caso, simplemente alimentarás los datos (y los metadatos) a la clase [AnalyzeAndTransformDataset](https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam/AnalyzeAndTransformDataset) y utilizarás la función de preprocesamiento que definiste anteriormente para transformar los datos.\n",
    "\n",
    "Para conocer mejor la sintaxis de Beam para las tuberías de transformación, puede consultar la documentación [aquí](https://beam.apache.org/documentation/programming-guide/#applying-transforms) y probar el breve Colab [aquí](https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/get-started/try-apache-beam-py.ipynb#scrollTo=J5HMFSzD8O2U).\n",
    "\n",
    "*Nota: Puede ignorar con seguridad la advertencia sobre argumentos no comparables que se muestra después de ejecutar la celda de abajo.\n",
    "\n",
    "\n",
    "<details><summary>Texto Original</summary>\n",
    "\n",
    "## Generate a constant graph with the required transformations\n",
    "\n",
    "Now you're ready to put everything together and transform your data. Like TFDV last week, Tensorflow Transform also uses [Apache Beam](https://beam.apache.org/) for deployment scalability and flexibility. As you'll see below, Beam uses the pipe (`|`) operator to stack the different stages of the pipeline. In this case, you will just feed the data (and metadata) to the [AnalyzeAndTransformDataset](https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam/AnalyzeAndTransformDataset) class and use the preprocessing function you defined above to transform the data.\n",
    "\n",
    "For a closer look at Beam syntax for tranform pipelines, you can refer to the documentation [here](https://beam.apache.org/documentation/programming-guide/#applying-transforms) and try the short Colab [here](https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/get-started/try-apache-beam-py.ipynb#scrollTo=J5HMFSzD8O2U).\n",
    "\n",
    "*Note: You can safely ignore the warning about unparseable args shown after running the cell below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mAF9w7RTZU7c"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/opt/conda/lib/python3.8/site-packages/ipykernel_launcher.py', '-f', '/home/jovyan/.local/share/jupyter/runtime/kernel-48a1dfb7-0bc2-4546-a3d3-40f7c958124e.json']\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw data:\n",
      "[{'s': 'hello', 'x': 1, 'y': 1},\n",
      " {'s': 'world', 'x': 2, 'y': 2},\n",
      " {'s': 'hello', 'x': 3, 'y': 3}]\n",
      "\n",
      "Transformed data:\n",
      "[{'s_integerized': 0,\n",
      "  'x_centered': -1.0,\n",
      "  'x_centered_times_y_normalized': -0.0,\n",
      "  'y_normalized': 0.0},\n",
      " {'s_integerized': 1,\n",
      "  'x_centered': 0.0,\n",
      "  'x_centered_times_y_normalized': 0.0,\n",
      "  'y_normalized': 0.5},\n",
      " {'s_integerized': 0,\n",
      "  'x_centered': 1.0,\n",
      "  'x_centered_times_y_normalized': 1.0,\n",
      "  'y_normalized': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "# Ignore the warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# a temporary directory is needed when analyzing the data\n",
    "with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "    \n",
    "    # define the pipeline using Apache Beam syntax\n",
    "    transformed_dataset, transform_fn = (\n",
    "        \n",
    "        # analyze and transform the dataset using the preprocessing function\n",
    "        (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(\n",
    "            preprocessing_fn)\n",
    "    )\n",
    "\n",
    "# unpack the transformed dataset\n",
    "transformed_data, transformed_metadata = transformed_dataset\n",
    "\n",
    "# print the results\n",
    "print('\\nRaw data:\\n{}\\n'.format(pprint.pformat(raw_data)))\n",
    "print('Transformed data:\\n{}'.format(pprint.pformat(transformed_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO6LyTneNndy"
   },
   "source": [
    "## ¿Es esta la respuesta correcta?\n",
    "Anteriormente, se utilizaba `tf.Transform` para hacer esto:\n",
    "```\n",
    "x_centrado = x - tft.mean(x)\n",
    "y_normalizado = tft.scale_to_0_1(y)\n",
    "s_integrada = tft.compute_and_apply_vocabulary(s)\n",
    "x_centrado_tiempo_y_normalizado = (x_centrado * y_normalizado)\n",
    "```\n",
    "#### x_centered\n",
    "Con la entrada de `[1, 2, 3]` la media de `x` es 2, y la restas de `x` para centrar tus valores de `x` en 0. Así que el resultado de `[-1.0, 0.0, 1.0]` es correcto.\n",
    "#### y_normalized\n",
    "A continuación, has escalado los valores de `y` entre 0 y 1. Tu entrada fue `[1, 2, 3]` por lo que el resultado de `[0.0, 0.5, 1.0]` es correcto.\n",
    "#### s_integerized\n",
    "Has mapeado tus cadenas a índices en un vocabulario, y sólo había 2 palabras en tu vocabulario (\"hola\" y \"mundo\").  Así que con la entrada de `[\"hola\", \"mundo\", \"hola\"]` el resultado de `[0, 1, 0]` es correcto.\n",
    "#### x_centered_times_y_normalized\n",
    "Has creado una nueva función cruzando `x_centrado` y `y_normalizado` utilizando la multiplicación.  Ten en cuenta que esto multiplica los resultados, no los valores originales, y el nuevo resultado de `[-0.0, 0.0, 1.0]` es correcto.\n",
    "\n",
    "\n",
    "<details><summary>Texto Original</summary>\n",
    "\n",
    "## Is this the right answer?\n",
    "Previously, you used `tf.Transform` to do this:\n",
    "```\n",
    "x_centered = x - tft.mean(x)\n",
    "y_normalized = tft.scale_to_0_1(y)\n",
    "s_integerized = tft.compute_and_apply_vocabulary(s)\n",
    "x_centered_times_y_normalized = (x_centered * y_normalized)\n",
    "```\n",
    "#### x_centered\n",
    "With input of `[1, 2, 3]` the mean of `x` is 2, and you subtract it from `x` to center your `x` values at 0.  So the result of `[-1.0, 0.0, 1.0]` is correct.\n",
    "#### y_normalized\n",
    "Next, you scaled your `y` values between 0 and 1.  Your input was `[1, 2, 3]` so the result of `[0.0, 0.5, 1.0]` is correct.\n",
    "#### s_integerized\n",
    "You mapped your strings to indexes in a vocabulary, and there were only 2 words in your vocabulary (\"hello\" and \"world\").  So with input of `[\"hello\", \"world\", \"hello\"]` the result of `[0, 1, 0]` is correct.\n",
    "#### x_centered_times_y_normalized\n",
    "You created a new feature by crossing `x_centered` and `y_normalized` using multiplication.  Note that this multiplies the results, not the original values, and the new result of `[-0.0, 0.0, 1.0]` is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkMssqfDdODY"
   },
   "source": [
    "### Resumen\n",
    "\n",
    "En este laboratorio, usted pasó por los fundamentos de la utilización de Tensorflow Transform para convertir los datos en bruto en características. Este código puede ser utilizado para transformar tanto los datos de entrenamiento como los de servicio. Sin embargo, el código puede ser bastante complejo si usted va a utilizar esto como una biblioteca independiente para construir una tubería (ver este [cuaderno](https://www.tensorflow.org/tfx/tutorials/transform/census) como referencia). Ahora que sabes lo que ocurre bajo el capó, puedes utilizar un conjunto de herramientas de alto nivel como [Tensorflow Extended](https://www.tensorflow.org/tfx) para simplificar el proceso. Esto abstraerá algunos de los pasos que hiciste aquí, como definir manualmente los esquemas y usar las funciones `tft_beam`. También aprovechará otras librerías, como TFDV, para realizar otros procesos en el pipeline de aprendizaje automático habitual como la detección de anomalías. Podrás ver esto en el próximo laboratorio.\n",
    "\n",
    "<details><summary>Texto Original</summary>\n",
    "\n",
    "### Wrap Up\n",
    "\n",
    "In this lab, you went through the fundamentals of using Tensorflow Transform to turn raw data into features. This code can be used to transform both the training and serving data. However, the code can be quite complex if you'll be using this as a standalone library to build a pipeline (see this [notebook](https://www.tensorflow.org/tfx/tutorials/transform/census) for reference). Now that you know what is going on under the hood, you can use a higher-level set of tools like [Tensorflow Extended](https://www.tensorflow.org/tfx) to simplify the process. This will abstract some of the steps you did here like manually defining schemas and using `tft_beam` functions. It will also leverage other libraries, such as TFDV, to perform other processes in the usual machine learning pipeline like detecting anomalies. You will get to see these in the next lab."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tghWegsjhpkt"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
