{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IrnqNnWO04A"
      },
      "source": [
        "# Ungraded Lab: Feature Engineering with Accelerometer Data\n",
        "\n",
        "Este cuaderno demuestra cómo preparar datos de series temporales tomados de un acelerómetro. Para este ejemplo utilizaremos el [WISDM Human Activity Recognition Dataset](http://www.cis.fordham.edu/wisdm/dataset.php). Este conjunto de datos puede utilizarse para predecir la actividad que realiza un usuario a partir de un conjunto de valores de aceleración registrados desde el acelerómetro de un smartphone.\n",
        "\n",
        "El conjunto de datos consta de datos del acelerómetro en los ejes x, y y z registrados para 36 usuarios diferentes. Un total de 6 actividades: \"Caminar\", \"Correr\", \"Subir escaleras\", \"Bajar escaleras\", \"Sentarse\" y \"Estar de pie\". Los sensores tienen una frecuencia de muestreo de 20 Hz, lo que significa que se registran 20 observaciones por segundo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDhCGkJoQDEN"
      },
      "source": [
        "## Install Packages\n",
        "\n",
        "As with the previous lab, you will install the `tensorflow_transform` Python package and its dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qONoNgHNWKt3"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_transform==1.4.0\n",
        "!pip install apache-beam==2.39.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvoAuLGMQXhL"
      },
      "source": [
        "**Nota: En Google Colab, es necesario reiniciar el tiempo de ejecución en este punto para finalizar la actualización de los paquetes que acaba de instalar. Puede hacerlo haciendo clic en el botón \"Reiniciar el tiempo de ejecución\" al final de la celda de salida anterior (después de la instalación), o seleccionando \"Tiempo de ejecución > Reiniciar el tiempo de ejecución\" en la barra de menús. **Por favor, no pases a la siguiente sección sin reiniciar.** También puedes ignorar los errores de incompatibilidad de versiones de algunos de los paquetes incluidos porque no los usaremos en este cuaderno.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyHZtPotQhL0"
      },
      "source": [
        "## Imports\n",
        "\n",
        "La ejecución de las importaciones que se indican a continuación no debería mostrar ningún error. De lo contrario, reinicie su tiempo de ejecución o vuelva a ejecutar la celda de instalación del paquete anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5isfzOcWbCl"
      },
      "outputs": [],
      "source": [
        "import apache_beam as beam\n",
        "print('Apache Beam version: {}'.format(beam.__version__))\n",
        "\n",
        "import tensorflow as tf\n",
        "print('Tensorflow version: {}'.format(tf.__version__))\n",
        "\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow_transform import beam as tft_beam\n",
        "from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "print('TensorFlow Transform version: {}'.format(tft.__version__))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll_BqMx4QyP5"
      },
      "source": [
        "## Download the Data\n",
        "\n",
        "A continuación, descargará los datos y los colocará en su espacio de trabajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu4uJ4RgEfE8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Directorio de los archivos de datos brutos\n",
        "DATA_DIR = '/content/data/'\n",
        "\n",
        "# Descargar el conjunto de datos\n",
        "!wget -nc https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/raw/main/course2/week4-ungraded-lab/data/WISDM_ar_latest.tar.gz -P {DATA_DIR}\n",
        "\n",
        "# Extraer el conjunto de datos\n",
        "!tar -xvf {DATA_DIR}/WISDM_ar_latest.tar.gz -C {DATA_DIR}\n",
        "\n",
        "# Asignar una ruta de datos a una variable para facilitar su consulta\n",
        "INPUT_FILE = os.path.join(DATA_DIR, 'WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKDJaZfEQ1yT"
      },
      "source": [
        "## Inspect the Data\n",
        "\n",
        "You should now inspect the dataset and you can start by previewing it as a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MipCmynFWD47"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Poner el conjunto de datos en un marco de datos\n",
        "df = pd.read_csv(INPUT_FILE, header=None, names=['user_id', 'activity', 'timestamp', 'x-acc','y-acc', 'z-acc'])\n",
        "\n",
        "# Vista previa de las primeras filas\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuHH4Ffdz0wE"
      },
      "source": [
        "Es posible que note el punto y coma al final de los elementos `z-acc`. Esto puede hacer que los elementos se traten como una cadena, por lo que puede querer eliminarlo cuando analice sus datos. Esto lo hará más adelante en el proceso con [Beam.map()](https://beam.apache.org/documentation/transforms/python/elementwise/map/). De esto también se encarga la función `visualize_plots()` que se utilizará en la siguiente sección."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK9tAyLVCgTe"
      },
      "outputs": [],
      "source": [
        "# Visulaization Utilities\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_value_plots_for_categorical_feature(feature, colors=['b']):\n",
        "    '''Plots a bar graph for categorical features'''\n",
        "    counts = feature.value_counts()\n",
        "    plt.bar(counts.index, counts.values, color=colors)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_plots(dataset, activity, columns):\n",
        "    '''Visualizes the accelerometer data against time'''\n",
        "    features = dataset[dataset['activity'] == activity][columns][:200]\n",
        "    if 'z-acc' in columns:\n",
        "        # remove semicolons in the z-acc column\n",
        "        features['z-acc'] = features['z-acc'].replace(regex=True, to_replace=r';', value=r'')\n",
        "        features['z-acc'] = features['z-acc'].astype(np.float64)\n",
        "    axis = features.plot(subplots=True, figsize=(16, 12), \n",
        "                     title=activity)\n",
        "\n",
        "    for ax in axis:\n",
        "        ax.legend(loc='lower left', bbox_to_anchor=(1.0, 0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvdKe6NP3EL"
      },
      "source": [
        "### Histogram of Activities\n",
        "\n",
        "Ahora puede proceder a las visualizaciones. Puede trazar el histograma de actividades y hacer sus observaciones. Por ejemplo, te darás cuenta de que hay más datos para caminar y trotar que para otras actividades. Esto puede tener un efecto en la forma en que su modelo aprende cada actividad, por lo que debe tomar nota de ello. Por ejemplo, puede que quieras recoger más datos para las otras actividades."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dva7JfmGDGQT"
      },
      "outputs": [],
      "source": [
        "# Trazar el histograma de actividades\n",
        "visualize_value_plots_for_categorical_feature(df['activity'], colors=['r', 'g', 'b', 'y', 'm', 'c'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj1bf2WZQHN8"
      },
      "source": [
        "### Histograma de mediciones por usuario\n",
        "También puede observar el número de mediciones realizadas por usuario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jJhG5DMFj-d"
      },
      "outputs": [],
      "source": [
        "# Plot the histogram for users\n",
        "visualize_value_plots_for_categorical_feature(df['user_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6CeWHKg3JC3"
      },
      "source": [
        "Puedes consultar con los expertos en la materia cuál de los usuarios debe formar parte del conjunto de entrenamiento o de prueba. Para este laboratorio, sólo harás una simple partición. Pondrás los identificadores de usuario del 1 al 30 en el conjunto de entrenamiento, y el resto en el conjunto de prueba. Aquí está el `partition_fn` que usarás para `Beam.Partition()` más tarde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n800M3nz3DFh"
      },
      "outputs": [],
      "source": [
        "def partition_fn(line, num_partitions):\n",
        "  '''\n",
        "  Función de partición para trabajar con Beam.partition\n",
        "\n",
        "  Args:\n",
        "    line (string) - Un registro en el archivo CSV.\n",
        "    num_partition (integer) - Número de particiones. Argumento requerido por Beam. No se utiliza en esta función.\n",
        "\n",
        "  Devuelve:\n",
        "    0 o 1 (integer) - 0 si el ID de usuario es menor de 30, 1 en caso contrario. \n",
        "  '''\n",
        "  \n",
        "  # Obtiene la primera subcadena delimitada por una coma. Convertir en un int.\n",
        "  user_id = int(line[:line.index(b',')])\n",
        "\n",
        "  # Comprueba si está por encima o por debajo de 30\n",
        "  partition_num = int(user_id <= 30)\n",
        "\n",
        "  return partition_num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY9wwg10QrXo"
      },
      "source": [
        "### Acceleration per Activity\n",
        "\n",
        "Por último, puedes trazar las mediciones de los sensores frente a las marcas de tiempo. Puedes observar que la aceleración es mayor en actividades como el jogging en comparación con estar sentado, lo que debería ser el comportamiento esperado. Si este no es el caso, entonces puede haber problemas con el sensor y puede hacer que los datos no sean válidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLa8UAe3JDZ3"
      },
      "outputs": [],
      "source": [
        "# Plot the measurements for `Jogging`\n",
        "visualize_plots(df, 'Jogging', columns=['x-acc', 'y-acc', 'z-acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1CvWGpEQwKO"
      },
      "outputs": [],
      "source": [
        "visualize_plots(df, 'Sitting', columns=['x-acc', 'y-acc', 'z-acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjZDkX-L4s-B"
      },
      "source": [
        "## Declare Schema for Cleaned Data\n",
        "\n",
        "Como siempre, querrá declarar un esquema para asegurarse de que su entrada de datos se analiza correctamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeUDSzCr4y5F"
      },
      "outputs": [],
      "source": [
        "STRING_FEATURES = ['activity']\n",
        "INT_FEATURES = ['user_id', 'timestamp']\n",
        "FLOAT_FEATURES = ['x-acc', 'y-acc', 'z-acc']\n",
        "\n",
        "# Declare feature spec\n",
        "RAW_DATA_FEATURE_SPEC = dict(\n",
        "    [(name, tf.io.FixedLenFeature([], tf.string))\n",
        "     for name in STRING_FEATURES] +\n",
        "    [(name, tf.io.FixedLenFeature([], tf.int64))\n",
        "     for name in INT_FEATURES] +\n",
        "    [(name, tf.io.FixedLenFeature([], tf.float32))\n",
        "     for name in FLOAT_FEATURES]\n",
        ")\n",
        "\n",
        "# Create schema from feature spec\n",
        "RAW_DATA_SCHEMA = tft.tf_metadata.schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXlu3wOBuWor"
      },
      "source": [
        "## Create a `tf.Transform` preprocessing_fn\n",
        "\n",
        "A continuación, puede definir su función de preprocesamiento. Para este ejercicio, escalarás las características flotantes por sus valores min-max y crearás una búsqueda de vocabulario para la etiqueta de la cadena. También descartará las características que no necesitará en el modelo, como el ID de usuario y la marca de tiempo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neDQbbY30Sdu"
      },
      "outputs": [],
      "source": [
        "LABEL_KEY = 'activity'\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
        "\n",
        "  # Copiar entradas\n",
        "  outputs = inputs.copy()\n",
        "\n",
        "  # Eliminar las características que no deben incluirse como entradas en el modelo\n",
        "  del outputs[\"user_id\"]\n",
        "  del outputs[\"timestamp\"]\n",
        "  \n",
        "  # Crear un vocabulario para las etiquetas de las cadenas\n",
        "  outputs[LABEL_KEY] = tft.compute_and_apply_vocabulary(inputs[LABEL_KEY],vocab_filename=LABEL_KEY)\n",
        "\n",
        "  # Escala las características por su mínimo-máximo\n",
        "  for key in FLOAT_FEATURES:\n",
        "     outputs[key] = tft.scale_by_min_max(outputs[key])\n",
        "\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SSRMC37SkyI"
      },
      "source": [
        "## Transform the data\n",
        "\n",
        "Ahora estás listo para empezar a transformar los datos en un pipeline de Apache Beam usando Tensorflow Transform. Seguirá los siguientes pasos principales:\n",
        "\n",
        "1. Lee los datos usando `beam.io.ReadFromText`.\n",
        "1. 2. Limpiarlos usando `beam.Map` y `beam.Filter`.\n",
        "1. 2. Transformarlos usando un pipeline de preprocesamiento que escala los datos numéricos y convierte los datos categóricos de cadenas a índices de valores int64. \n",
        "1. Escribe el resultado como un `TFRecord` de protos `Example`, que puede ser utilizado para entrenar un modelo más tarde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ_eE8nntSSs"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from tfx_bsl.coders.example_coder import RecordBatchToExamplesEncoder\n",
        "from tfx_bsl.public import tfxio\n",
        "\n",
        "# Nombres de directorios para las salidas de TF Transform\n",
        "WORKING_DIR = 'transform_dir'\n",
        "TRANSFORM_TRAIN_FILENAME = 'transform_train'\n",
        "TRANSFORM_TEST_FILENAME = 'transform_test'\n",
        "TRANSFORM_TEMP_DIR = 'tft_temp'\n",
        "\n",
        "ordered_columns = ['user_id', 'activity', 'timestamp', 'x-acc','y-acc', 'z-acc']\n",
        "\n",
        "def transform_data(working_dir):\n",
        "    '''\n",
        "    Reads a CSV File and preprocesses the data using TF Transform\n",
        "\n",
        "    Args:\n",
        "      working_dir (string) - directory to place TF Transform outputs\n",
        "    \n",
        "    Returns:\n",
        "      transform_fn - transformation graph\n",
        "      transformed_train_data - transformed training examples\n",
        "      transformed_test_data - transformed test examples\n",
        "      transformed_metadata - transform output metadata\n",
        "    '''\n",
        "\n",
        "    # Eliminar TF Transform si ya existe\n",
        "    if os.path.exists(working_dir):\n",
        "      shutil.rmtree(working_dir)\n",
        "\n",
        "    with beam.Pipeline() as pipeline:\n",
        "        with tft_beam.Context(temp_dir=os.path.join(working_dir, TRANSFORM_TEMP_DIR)):\n",
        "  \n",
        "          # Leer el CSV de entrada, limpiar y filtrar los datos (sustituir el punto y coma y las filas incompletas)\n",
        "          raw_data = (\n",
        "              pipeline\n",
        "              | 'ReadTrainData' >> beam.io.ReadFromText(INPUT_FILE, coder=beam.coders.BytesCoder())\n",
        "              | 'CleanLines' >> beam.Map(lambda line: line.replace(b',;', b'').replace(b';', b''))\n",
        "              | 'FilterLines' >> beam.Filter(lambda line: line.count(b',') == len(ordered_columns) - 1 and line[-1:] != b','))\n",
        "\n",
        "          # Partición de los datos en datos de entrenamiento y de prueba utilizando beam.Partition\n",
        "          raw_train_data, raw_test_data = (raw_data\n",
        "                                  | 'TrainTestSplit' >> beam.Partition(partition_fn, 2))\n",
        "                    \n",
        "          # Crear un TFXIO para leer los datos con el esquema. \n",
        "          csv_tfxio = tfxio.BeamRecordCsvTFXIO(\n",
        "              physical_format='text',\n",
        "              column_names=ordered_columns,\n",
        "              schema=RAW_DATA_SCHEMA)\n",
        "\n",
        "          # Analizar los datos brutos del tren en entradas para la Transformación TF\n",
        "          raw_train_data = (raw_train_data \n",
        "                            | 'DecodeTrainData' >> csv_tfxio.BeamSource())\n",
        "\n",
        "          # Obtener los metadatos de los datos brutos\n",
        "          RAW_DATA_METADATA = csv_tfxio.TensorAdapterConfig()\n",
        "          \n",
        "          # Emparejar los datos de la prueba con los metadatos en una tupla\n",
        "          raw_train_dataset = (raw_train_data, RAW_DATA_METADATA)\n",
        "\n",
        "          # Transformación de los datos de entrenamiento. El formato de salida TFXIO (RecordBatch)\n",
        "          # se elige para mejorar el rendimiento.\n",
        "          (transformed_train_data,transformed_metadata) , transform_fn = (\n",
        "              raw_train_dataset \n",
        "                | 'AnalyzeAndTransformTrainData' >> tft_beam.AnalyzeAndTransformDataset(preprocessing_fn, output_record_batches=True))\n",
        "          \n",
        "          # # Analizar los datos de prueba en bruto en entradas para TF Transform\n",
        "          raw_test_data = (raw_test_data \n",
        "                            |'DecodeTestData' >> csv_tfxio.BeamSource())\n",
        "\n",
        "          # Emparejar los datos de la prueba con los metadatos en una tupla\n",
        "          raw_test_dataset = (raw_test_data, RAW_DATA_METADATA)\n",
        "\n",
        "          # Ahora aplique la misma función de transformación a los datos de prueba.\n",
        "          # No necesitas el esquema de datos transformados. Es el mismo que antes.pñllllllllllllllllllllllllllllllllllllll\n",
        "          transformed_test_data, _ = ((raw_test_dataset, transform_fn) \n",
        "                                        | 'AnalyzeAndTransformTestData' >> tft_beam.TransformDataset(output_record_batches=True))\n",
        "          \n",
        "          # Declare an encoder to convert output record batches to TF Examples \n",
        "          transformed_data_coder = RecordBatchToExamplesEncoder(transformed_metadata.schema)\n",
        "\n",
        "          \n",
        "          # Encode transformed train data and write to disk\n",
        "          _ = (\n",
        "              transformed_train_data\n",
        "              | 'EncodeTrainData' >> beam.FlatMapTuple(lambda batch, _: transformed_data_coder.encode(batch))\n",
        "              | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n",
        "                  os.path.join(working_dir, TRANSFORM_TRAIN_FILENAME)))\n",
        "\n",
        "          # Encode transformed test data and write to disk\n",
        "          _ = (\n",
        "              transformed_test_data\n",
        "              | 'EncodeTestData' >> beam.FlatMapTuple(lambda batch, _: transformed_data_coder.encode(batch))\n",
        "              | 'WriteTestData' >> beam.io.WriteToTFRecord(\n",
        "                  os.path.join(working_dir, TRANSFORM_TEST_FILENAME)))\n",
        "          \n",
        "          # Write transform function to disk\n",
        "          _ = (\n",
        "            transform_fn\n",
        "            | 'WriteTransformFn' >>\n",
        "            tft_beam.WriteTransformFn(os.path.join(working_dir)))\n",
        "\n",
        "    return transform_fn, transformed_train_data, transformed_test_data, transformed_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GPbh3qKzyTb"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  return transform_data(WORKING_DIR)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  transform_fn, transformed_train_data,transformed_test_data, transformed_metadata = main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CqjN6qrTzQ5"
      },
      "source": [
        "##Prepare Training and Test Datasets from TFTransformOutput\n",
        "\n",
        "Ahora que tiene los ejemplos transformados, necesita preparar las ventanas del conjunto de datos para estos datos de series temporales. Como se discutió en clase, usted quiere agrupar una serie de mediciones y eso será la característica para una etiqueta particular. En este caso particular, tiene sentido agrupar mediciones consecutivas y usar eso como el indicador de una actividad. Por ejemplo, si se toman 80 mediciones y se oscila mucho (al igual que en las visualizaciones en las partes anteriores de este cuaderno), entonces el modelo debe ser capaz de decir que se trata de una actividad \"Corriendo\". Vamos a implementar esto en las siguientes celdas utilizando el método [tf.data.Dataset.window()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window). Observa que hay una función `add_mode()`. Si recuerdas el aspecto del CSV original, verás que cada fila tiene una etiqueta de actividad. Por lo tanto, si queremos asociar una sola actividad a un grupo de 80 mediciones, entonces simplemente obtenemos la actividad que más se menciona en todas esas filas (por ejemplo, si 75 elementos de la ventana apuntan a la actividad `Caminando` y sólo 5 apuntan a `Sentado`, entonces toda la ventana se asocia a `Caminando`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQxYFAXkNUwP"
      },
      "outputs": [],
      "source": [
        "# Obtener la salida del componente Transformación\n",
        "tf_transform_output = tft.TFTransformOutput(os.path.join(WORKING_DIR))\n",
        "\n",
        "# Parameters\n",
        "HISTORY_SIZE = 80\n",
        "BATCH_SIZE = 100\n",
        "STEP_SIZE = 40\n",
        "\n",
        "def parse_function(example_proto):\n",
        "    '''Parse the values from tf examples'''\n",
        "    feature_spec = tf_transform_output.transformed_feature_spec()\n",
        "    features = tf.io.parse_single_example(example_proto, feature_spec)\n",
        "    values = list(features.values())\n",
        "    values = [float(value) for value in values]\n",
        "    features = tf.stack(values, axis=0)\n",
        "    return features\n",
        "\n",
        "def add_mode(features):\n",
        "    '''Calculate mode of activity for the current history size of elements'''\n",
        "    unique, _, count = tf.unique_with_counts(features[:,0])\n",
        "    max_occurrences = tf.reduce_max(count)\n",
        "    max_cond = tf.equal(count, max_occurrences)\n",
        "    max_numbers = tf.squeeze(tf.gather(unique, tf.where(max_cond)))\n",
        "\n",
        "    #Características (X) son todas las características excepto la actividad (x-acc, y-acc, z-acc)\n",
        "    #Objetivo(Y) es el modo de los valores de actividad de todas las filas de esta ventana\n",
        "    return (features[:,1:], max_numbers)\n",
        "\n",
        "def get_windowed_dataset(path):\n",
        "  '''Get the dataset and group them into windows'''\n",
        "  dataset = tf.data.TFRecordDataset(path)\n",
        "  dataset = dataset.map(parse_function)\n",
        "  dataset = dataset.window(HISTORY_SIZE, shift=STEP_SIZE, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda window: window.batch(HISTORY_SIZE))\n",
        "  dataset = dataset.map(add_mode)\n",
        "  dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  dataset = dataset.repeat()\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0grFNuaTsk3"
      },
      "outputs": [],
      "source": [
        "# Obtiene la lista de nombres de archivos de datos de entrenamiento y de prueba tfrecord de las salidas de la transformación\n",
        "train_tfrecord_files = tf.io.gfile.glob(os.path.join(WORKING_DIR, TRANSFORM_TRAIN_FILENAME + '*'))\n",
        "test_tfrecord_files = tf.io.gfile.glob(os.path.join(WORKING_DIR, TRANSFORM_TEST_FILENAME + '*'))\n",
        "\n",
        "# Generate dataset windows\n",
        "windowed_train_dataset = get_windowed_dataset(train_tfrecord_files[0])\n",
        "windowed_test_dataset = get_windowed_dataset(test_tfrecord_files[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89KG2jANSWt4"
      },
      "outputs": [],
      "source": [
        "# Vista previa de un ejemplo en el conjunto de datos de entrenamiento\n",
        "for x, y in windowed_train_dataset.take(1):\n",
        "  print(\"\\nFeatures (x-acc, y-acc, z-acc):\\n\")\n",
        "  print(x)\n",
        "  print(\"\\nTarget (activity):\\n\")\n",
        "  print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOXmSO4YTRWv"
      },
      "source": [
        "Debería ver una muestra de la ventana de un conjunto de datos en la parte superior. Hay 80 mediciones consecutivas de `x-acc`, `y-acc` y `z-acc` que corresponden a una sola actividad etiquetada. Además, también se ha configurado para que haya lotes de 100 ventanas. Esto puede ser alimentado para entrenar un LSTM para que pueda aprender a detectar actividades basadas en 80 ventanas de medición. También se puede previsualizar una muestra en el conjunto de pruebas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klu7t6QGlhSc"
      },
      "outputs": [],
      "source": [
        "# Preview an example in the train dataset\n",
        "for x, y in windowed_test_dataset.take(1):\n",
        "  print(\"\\nFeatures (x-acc, y-acc, z-acc):\\n\")\n",
        "  print(x)\n",
        "  print(\"\\nTarget (activity):\\n\")\n",
        "  print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baQ6Blk0Tire"
      },
      "source": [
        "## Wrap Up\n",
        "\n",
        "En este laboratorio, ha podido preparar datos de series temporales de un acelerómetro para transformar características que se agrupan en ventanas para hacer predicciones. El mismo concepto puede aplicarse a cualquier dato en el que necesites tomar unos segundos de mediciones antes de que el modelo haga una predicción. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "C2_W4_Lab_2_Signals.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}