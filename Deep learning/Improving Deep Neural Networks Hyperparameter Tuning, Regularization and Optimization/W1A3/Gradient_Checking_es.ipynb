{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprobación del gradiente\n",
    "\n",
    "Bienvenidos a la última tarea de esta semana. En esta tarea implementarás la comprobación de gradientes.\n",
    "\n",
    "Al final de este cuaderno, serás capaz de:\n",
    "\n",
    "Implementar la comprobación de gradientes para verificar la exactitud de tu implementación de backprop.\n",
    "\n",
    "## Nota importante sobre el envío al AutoGrader\n",
    "\n",
    "Antes de enviar su tarea al AutoGrader, por favor asegúrese de que no está haciendo lo siguiente:\n",
    "\n",
    "1. 1. No ha añadido ninguna declaración _extra_ `print` en la tarea.\n",
    "2. 2. No ha añadido ninguna celda de código _extra_ en la tarea.\n",
    "3. No ha cambiado ningún parámetro de la función.\n",
    "4. No ha utilizado ninguna variable global dentro de sus ejercicios calificados. A menos que se le indique específicamente que lo haga, por favor absténgase de hacerlo y utilice las variables locales en su lugar.\n",
    "5. 5. No está cambiando el código de asignación donde no es necesario, como la creación de variables _extra_.\n",
    "\n",
    "Si hace algo de lo siguiente, obtendrá un error como `Grader no encontrado` (o similarmente inesperado) al enviar su tarea. Antes de pedir ayuda/depurar los errores de su tarea, compruebe esto primero. Si este es el caso, y no recuerda los cambios que ha realizado, puede obtener una nueva copia de la tarea siguiendo estas [instrucciones](https://www.coursera.org/learn/deep-neural-network/supplement/QWEnZ/h-ow-to-refresh-your-workspace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Problem Statement](#2)\n",
    "- [3 - How does Gradient Checking work?](#3)\n",
    "- [4 - 1-Dimensional Gradient Checking](#4)\n",
    "    - [Exercise 1 - forward_propagation](#ex-1)\n",
    "    - [Exercise 2 - backward_propagation](#ex-2)\n",
    "    - [Exercise 3 - gradient_check](#ex-3)\n",
    "- [5 - N-Dimensional Gradient Checking](#5)\n",
    "    - [Exercise 4 - gradient_check_n](#ex-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from testCases import *\n",
    "from public_tests import *\n",
    "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Planteamiento del problema\n",
    "\n",
    "Formas parte de un equipo que trabaja para que los pagos por móvil estén disponibles en todo el mundo, y se te pide que construyas un modelo de aprendizaje profundo para detectar el fraude: cada vez que alguien hace un pago, quieres ver si el pago puede ser fraudulento, como por ejemplo si la cuenta del usuario ha sido tomada por un hacker.\n",
    "\n",
    "Ya sabes que la retropropagación es bastante difícil de implementar, y a veces tiene errores. Como se trata de una aplicación de misión crítica, el director general de su empresa quiere estar realmente seguro de que su implementación de la retropropagación es correcta. Tu director general dice: \"¡Dame pruebas de que tu retropropagación funciona realmente!\". Para dar esta seguridad, vas a utilizar la \"comprobación de gradiente\".\n",
    "\n",
    "¡Vamos a hacerlo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pero tenemos que encontrar una manera par<a name='3'></a>\n",
    "## 3 - ¿Cómo funciona la comprobación de gradientes?\n",
    "La propagación hacia atrás calcula los gradientes $\\frac{\\partial J}{\\partial \\theta}$, donde $\\theta$ denota los parámetros del modelo. $J$ se calcula utilizando la propagación hacia adelante y su función de pérdida.\n",
    "\n",
    "Debido a que la propagación hacia adelante es relativamente fácil de implementar, usted está seguro de que lo hizo bien, y por lo que son casi 100% seguro de que está calculando el costo $J $ correctamente. Por lo tanto, usted puede utilizar su código para el cálculo de $ J $ para verificar el código para el cálculo de $\\frac{\\partial J}{\\partial \\theta}$.\n",
    "\n",
    "Volvamos a la definición de una derivada (o gradiente):$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "Si no estás familiarizado con la notación \"$\\displaystyle \\lim_{\\varepsilon \\to 0}$\", es sólo una forma de decir \"cuando $\\varepsilon$ es muy, muy pequeño\".\n",
    "\n",
    "Usted sabe lo siguiente:\n",
    "\n",
    "\n",
    "$\\frac{\\partial J}{\\partial \\theta}$ es lo que quieres para asegurarte de que estás calculando correctamente.\n",
    "You can compute $J(\\theta + \\varepsilon)$ and $J(\\theta - \\varepsilon)$ (en el caso de que $\\theta$ isea un número real), ya que estás seguro de que tu implementación para $J$ es correcta.\n",
    "tilicemos la ecuación (1) y un valor pequeño para $\\varepsilon$ para convencer a su director general de que su código para calcular $\\frac{\\partial J}{\\partial \\theta}$ es correcto!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Comprobación del gradiente en 1 dimensión\n",
    "\n",
    "Consideremos una función lineal 1D $J(\\theta) = \\theta x$. El modelo contiene un solo parámetro de valor real $\\theta$, y toma $x$ como entrada.\n",
    "\n",
    "Se implementará un código para calcular $J(.)$ y su derivada $\\frac{parcial J}{parcial \\theta}$. A continuación, utilizará la comprobación del gradiente para asegurarse de que el cálculo de la derivada de $J$ es correcto. \n",
    "\n",
    "<img src=\"images/1Dgrad_kiank.png\" style=\"width:600px;height:250px;\">\n",
    "<caption><center><font color='purple'><b>Figure 1</b>:1D linear model </font></center></caption>\n",
    "\n",
    "El diagrama anterior muestra los pasos clave del cálculo: Primero empieza con $x$, luego evalúa la función $J(x)$ (\"propagación hacia delante\"). A continuación, calcular la derivada $\\frac{{parcial J}{parcial \\theta}$ (\"propagación hacia atrás\"). \n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - forward_propagation\n",
    "\n",
    "Implementa la `propagación hacia adelante`. Para esta función simple calcula $J(.)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Implementa la propagación lineal hacia adelante (calcula J) presentada en la Figura 1 (J(theta) = theta * x)\n",
    "    \n",
    "    Argumentos:\n",
    "    x -- una entrada de valor real\n",
    "    theta -- nuestro parámetro, un número real también\n",
    "    \n",
    "    Devuelve:\n",
    "    J -- el valor de la función J, calculado mediante la fórmula J(theta) = theta * x\n",
    "    \"\"\"\n",
    "    \n",
    "    # (aprox. 1 línea)\n",
    "    # J = = theta * x\n",
    "    # YOUR CODE STARTS HERE\n",
    "    J = theta * x\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = 8\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "J = forward_propagation(x, theta)\n",
    "print (\"J = \" + str(J))\n",
    "forward_propagation_test(forward_propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Ejercicio 2 - propagación_hacia_atrás\n",
    "\n",
    "Ahora, implementa el paso de `propagación hacia atrás` (cálculo de la derivada) de la Figura 1. Es decir, calcula la derivada de $J(\\theta) = \\theta x$ con respecto a $\\theta$. Para ahorrarte el cálculo, deberías obtener $dtheta = \\frac { \\partial J }{ \\partial \\theta} = x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation\n",
    "\n",
    "def backward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Calcula la derivada de J con respecto a theta (ver Figura 1).\n",
    "    \n",
    "    Argumentos:\n",
    "    x -- una entrada de valor real\n",
    "    theta -- nuestro parámetro, un número real también\n",
    "    \n",
    "    Devuelve:\n",
    "    dtheta -- el gradiente del coste con respecto a theta\n",
    "    \"\"\"\n",
    "    \n",
    "    # (approx. 1 line)\n",
    "    # dtheta = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    dtheta = x\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtheta = 2\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "dtheta = backward_propagation(x, theta)\n",
    "print (\"dtheta = \" + str(dtheta))\n",
    "backward_propagation_test(backward_propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-3'></a>\n",
    "### Ejercicio 3 - gradient_check\n",
    "\n",
    "Para demostrar que la función `propagación_hacia_atrás()` está calculando correctamente el gradiente $\\frac{{parcial J}{parcial \\theta}$, vamos a implementar la comprobación del gradiente.\n",
    "\n",
    "**Instrucciones**:\n",
    "- Primero calcule \"gradapprox\" usando la fórmula anterior (1) y un valor pequeño de $\\varepsilon$. Aquí están los pasos a seguir:\n",
    "    1. $\\theta^{+} = \\theta + \\varepsilon$\n",
    "    2. $\\theta^{-} = \\theta - \\varepsilon$\n",
    "    3. $J^{+} = J(\\theta^{+})$\n",
    "    4. $J^{-} = J(\\theta^{-})$\n",
    "    5. $gradapprox = \\frac{J^{+} - J^{-}{2 \\varepsilon}$\n",
    "- A continuación, calcular el gradiente utilizando la propagación hacia atrás, y almacenar el resultado en una variable \"grad\"\n",
    "- Por último, calcular la diferencia relativa entre \"gradapprox\" y el \"grad\" utilizando la siguiente fórmula\n",
    "\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2} $$\n",
    "\n",
    "Necesitarás 3 pasos para calcular esta fórmula:\n",
    "   - 1'. calcular el numerador utilizando np.linalg.norm(...)\n",
    "   - 2'. calcular el denominador. Tendrás que llamar a np.linalg.norm(...) dos veces.\n",
    "   - 3'. dividirlos.\n",
    "- Si esta diferencia es pequeña (digamos menos de $10^{-7}$), puedes estar bastante seguro de que has calculado tu gradiente correctamente. De lo contrario, puede haber un error en el cálculo del gradiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: gradient_check\n",
    "\n",
    "def gradient_check(x, theta, epsilon=1e-7, print_msg=False):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in Figure 1.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a float input\n",
    "    theta -- our parameter, a float as well\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient. Float output\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute gradapprox using left side of formula (1). epsilon is small enough, you don't need to worry about the limit.\n",
    "    # (approx. 5 lines)\n",
    "    # theta_plus =                                # Step 1\n",
    "    # theta_minus =                               # Step 2\n",
    "    # J_plus =                                    # Step 3\n",
    "    # J_minus =                                   # Step 4\n",
    "    # gradapprox =                                # Step 5\n",
    "    # YOUR CODE STARTS HERE\n",
    "    theta_plus  = theta + epsilon                # Step 1\n",
    "    theta_minus = theta - epsilon                # Step 2\n",
    "    J_plus      = theta_plus * x                 # Step 3\n",
    "    J_minus     = theta_minus * x                # Step 4\n",
    "    gradapprox  = (J_plus - J_minus)/(2*epsilon) # Step 5\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Check if gradapprox is close enough to the output of backward_propagation()\n",
    "    #(approx. 1 line) DO NOT USE \"grad = gradapprox\"\n",
    "    # grad =\n",
    "    # YOUR CODE STARTS HERE\n",
    "    grad = backward_propagation(x, theta)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    #(approx. 3 lines)\n",
    "    # numerator =                                 # Step 1'\n",
    "    # denominator =                               # Step 2'\n",
    "    # difference =                                # Step 3'\n",
    "    # YOUR CODE STARTS HERE\n",
    "    numerator   = np.linalg.norm(grad-gradapprox)                                # Step 1\n",
    "    denominator = np.linalg.norm(grad)+ np.linalg.norm(gradapprox)             # Step 2\n",
    "    difference  = numerator/denominator                                         # Step 3\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    if print_msg:\n",
    "        if difference > 2e-7:\n",
    "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 2.919335883291695e-10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(x, theta, print_msg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Felicidades, la diferencia es menor que el umbral de $10^{-7}$. Así que usted puede tener una alta confianza de que usted ha calculado correctamente el gradiente en `propagación hacia atrás() `. \n",
    "\n",
    "Ahora, en el caso más general, su función de coste $J$ tiene más de una sola entrada 1D. ¡Cuando usted está entrenando una red neuronal, $\\theta$ en realidad consiste en múltiples matrices $W^[l]}$ y sesgos $b^[l]}$! Es importante saber cómo hacer una comprobación de gradiente con entradas de mayor dimensión. ¡Hagámoslo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - N-Dimensional Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la siguiente figura describe la propagación hacia delante y hacia atrás de su modelo de detección de fraude.\n",
    "\n",
    "<img src=\"images/NDgrad_kiank.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center><font color='purple'><b>Figure 2</b>: Deep neural network. LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID</font></center></caption>\n",
    "\n",
    "Veamos sus implementaciones para la propagación hacia adelante y hacia atrás. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation (and computes the cost) presented in Figure 3.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- training set for m examples\n",
    "    Y -- labels for m examples \n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape (5, 4)\n",
    "                    b1 -- bias vector of shape (5, 1)\n",
    "                    W2 -- weight matrix of shape (3, 5)\n",
    "                    b2 -- bias vector of shape (3, 1)\n",
    "                    W3 -- weight matrix of shape (1, 3)\n",
    "                    b3 -- bias vector of shape (1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- the cost function (logistic cost for one example)\n",
    "    cache -- a tuple with the intermediate values (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve parameters\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "\n",
    "    # Cost\n",
    "    log_probs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
    "    cost = 1. / m * np.sum(log_probs)\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, ejecuta la propagación hacia atrás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input datapoint, of shape (input size, 1)\n",
    "    Y -- true \"label\"\n",
    "    cache -- cache output from forward_propagation_n()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1. / m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1. / m * np.dot(dZ2, A1.T) * 2\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1. / m * np.dot(dZ1, X.T)\n",
    "    db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha obtenido algunos resultados en el conjunto de pruebas de detección de fraudes, pero no está 100% seguro de su modelo. Nadie es perfecto. Implementemos la comprobación de gradientes para verificar si sus gradientes son correctos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo funciona la comprobación del gradiente?\n",
    "\n",
    "Al igual que en las secciones 3 y 4, se quiere comparar \"gradapprox\" con el gradiente calculado por retropropagación. La fórmula sigue siendo:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "Sin embargo, $\\theta$ ya no es un escalar. Es un diccionario llamado \"parámetros\". La función \"`diccionario_a_vector()`\" ha sido implementada para usted. Convierte el diccionario \"parámetros\" en un vector llamado \"valores\", que se obtiene al reestructurar todos los parámetros (W1, b1, W2, b2, W3, b3) en vectores y concatenarlos.\n",
    "\n",
    "La función inversa es \"`vector_a_diccionario`\" que devuelve el diccionario de \"parámetros\".\n",
    "\n",
    "<img src=\"images/dictionary_to_vector.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center><font color='purple'><b>Figure 2</b>: dictionary_to_vector() y vector_to_dictionary(). Necesitará estas funciones en gradient_check_n()</font></center></caption>\n",
    "\n",
    "El diccionario \"gradients\" también ha sido convertido en un vector \"grad\" usando gradients_to_vector(), así que no necesitas preocuparte por eso.\n",
    "\n",
    "Ahora, para cada parámetro de tu vector, aplicarás el mismo procedimiento que para el ejercicio gradient_check. Almacenarás cada aproximación del gradiente en un vector `gradapprox`. Si la comprobación va como se espera, cada valor de esta aproximación debe coincidir con los valores reales del gradiente almacenados en el vector `grad`. \n",
    "\n",
    "Ten en cuenta que `grad` se calcula utilizando la función `gradients_to_vector`, que utiliza las salidas de los gradientes de la función `backward_propagation_n`.\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Ejercicio 4 - gradient_check_n\n",
    "\n",
    "Implementa la función de abajo.\n",
    "\n",
    "**Instrucciones**: Aquí hay un pseudocódigo que te ayudará a implementar la comprobación del gradiente.\n",
    "\n",
    "Para cada i en num_parámetros:\n",
    "- Para calcular `J_plus[i]`:\n",
    "    1. Establecer $\\theta^{+}$ a `np.copy(parameters_values)`\n",
    "    2. Establecer $\\theta^{+}_i$ a $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Calcular $J^{+}_i$ utilizando a `forward_propagation_n(x, y, vector_to_dictionary(`$\\theta^{+}$ `))`.     \n",
    "- Para calcular `J_minus[i]`: hacer lo mismo con $\\theta^{-}$\n",
    "- Calcula $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "\n",
    "Así, se obtiene un vector gradapprox, donde gradapprox[i] es una aproximación del gradiente con respecto a `valores_de_parámetro[i]`. Ahora puedes comparar este vector gradapprox con el vector de gradientes de la retropropagación. Al igual que para el caso 1D (Pasos 1', 2', 3'), calcular: \n",
    "$$ diferencia = \\frac {\\| grad - gradapprox |_2}{\\| grad |_2 + \\| gradapprox |_2 } |tag{3}$$\n",
    "\n",
    "**Nota**: Utilice `np.linalg.norm` para obtener las normas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: gradient_check_n\n",
    "\n",
    "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7, print_msg=False):\n",
    "    \"\"\"\n",
    "    Comprueba si backward_propagation_n calcula correctamente el gradiente de la salida del coste por forward_propagation_n\n",
    "    \n",
    "    Argumentos:\n",
    "    parameters -- diccionario python que contiene sus parámetros \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "    grad -- salida de backward_propagation_n, contiene los gradientes del coste con respecto a los parámetros. \n",
    "    x -- punto de datos de entrada, de forma (tamaño de entrada, 1)\n",
    "    y -- verdadera \"etiqueta\"\n",
    "    epsilon -- pequeño desplazamiento de la entrada para calcular el gradiente aproximado con la fórmula(1)\n",
    "    \n",
    "    Devuelve:\n",
    "    diferencia -- diferencia (2) entre el gradiente aproximado y el gradiente de propagación hacia atrás\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set-up variables\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    \n",
    "    grad           = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus         = np.zeros((num_parameters, 1))\n",
    "    J_minus        = np.zeros((num_parameters, 1))\n",
    "    gradapprox     = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        \n",
    "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "        # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
    "        #(approx. 3 lines)\n",
    "        # theta_plus =                                        # Step 1\n",
    "        # theta_plus[i] =                                     # Step 2\n",
    "        # J_plus[i], _ =                                     # Step 3\n",
    "        # YOUR CODE STARTS HERE\n",
    "        theta_plus    = np.copy(parameters_values)                                     \n",
    "        theta_plus[i] = theta_plus[i] + epsilon                                  \n",
    "        J_plus[i], _  = forward_propagation_n( X, Y, vector_to_dictionary( theta_plus ))\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "        #(approx. 3 lines)\n",
    "        # theta_minus =                                    # Step 1\n",
    "        # theta_minus[i] =                                 # Step 2        \n",
    "        # J_minus[i], _ =                                 # Step 3\n",
    "        # YOUR CODE STARTS HERE\n",
    "        theta_minus    = np.copy(parameters_values)                            \n",
    "        theta_minus[i] = theta_minus[i] - epsilon                                 \n",
    "        J_minus[i], _  = forward_propagation_n( X, Y, vector_to_dictionary( theta_minus ))\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute gradapprox[i]\n",
    "        # (approx. 1 line)\n",
    "        # gradapprox[i] = \n",
    "        # YOUR CODE STARTS HERE\n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "    # (approx. 3 line)\n",
    "    # numerator =                                             # Step 1'\n",
    "    # denominator =                                           # Step 2'\n",
    "    # difference =                                            # Step 3'\n",
    "    # YOUR CODE STARTS HERE\n",
    "    numerator   = np.linalg.norm( ( grad-gradapprox )) \n",
    "    denominator = np.linalg.norm( grad ) + np.linalg.norm( gradapprox )\n",
    "    difference  = numerator / denominator \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    if print_msg:\n",
    "        if difference > 2e-7:\n",
    "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mThere is a mistake in the backward propagation! difference = 0.2850931567761623\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y, 1e-7, True)\n",
    "expected_values = [0.2850931567761623, 1.1890913024229996e-07]\n",
    "assert not(type(difference) == np.ndarray), \"You are not using np.linalg.norm for numerator or denominator\"\n",
    "assert np.any(np.isclose(difference, expected_values)), \"Wrong value. It is not one of the expected values\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>  <b> There is a mistake in the backward propagation!</b>  </td>\n",
    "        <td> difference = 0.2850931567761623 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Parece que había errores en el código de `backward_propagation_n`! Menos mal que has implementado la comprobación del gradiente. Vuelve a `propagación_hacia_atrás` y trata de encontrar/corregir los errores *(Pista: comprueba dW2 y db1)*. Vuelve a ejecutar la comprobación del gradiente cuando creas que lo has solucionado. Recuerda que tendrás que volver a ejecutar la celda que define `propagación_hacia_atrás_n()` si modificas el código. \n",
    "\n",
    "¿Puedes conseguir que la comprobación del gradiente declare que tu cálculo de la derivada es correcto? Aunque esta parte de la tarea no se califica, deberías intentar encontrar el error y volver a ejecutar la comprobación de gradiente hasta que estés convencido de que la retropropagación está ahora correctamente implementada. \n",
    "\n",
    "**Notas** \n",
    "- La comprobación del gradiente es lenta. Aproximar el gradiente con $\\frac{\\partial J}{\\partial \\theta} \\approx \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ es computacionalmente costoso. Por esta razón, no ejecutamos la comprobación del gradiente en cada iteración durante el entrenamiento. Sólo unas pocas veces para comprobar si el gradiente es correcto. \n",
    "- La comprobación del gradiente, al menos como la hemos presentado, no funciona con el abandono. Por lo general, se ejecuta el algoritmo de comprobación de gradiente sin dropout para asegurarse de que su backprop es correcto, a continuación, añadir dropout. \n",
    "\n",
    "Enhorabuena. Now you can be confident that your deep learning model for fraud detection is working correctly! You can even use this to convince your CEO. :) \n",
    "<br>\n",
    "<font color='blue'>\n",
    "    \n",
    "**Lo que debes recordar de este cuaderno**:\n",
    "- La comprobación del gradiente verifica la proximidad entre los gradientes de la retropropagación y la aproximación numérica del gradiente (calculada mediante la propagación hacia delante).\n",
    "- La comprobación del gradiente es lenta, por lo que no es conveniente ejecutarla en cada iteración del entrenamiento. Por lo general, se ejecuta sólo para asegurarse de que el código es correcto, luego se desactiva y se utiliza backprop para el proceso de aprendizaje real. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
