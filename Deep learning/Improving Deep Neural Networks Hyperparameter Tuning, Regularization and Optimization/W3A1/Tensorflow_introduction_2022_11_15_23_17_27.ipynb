{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a TensorFlow\n",
    "\n",
    "¡Bienvenidos a la tarea de programación de esta semana! Hasta ahora, siempre has utilizado Numpy para construir redes neuronales, pero esta semana explorarás un marco de aprendizaje profundo que te permite construir redes neuronales más fácilmente. Los marcos de aprendizaje automático como TensorFlow, PaddlePaddle, Torch, Caffe, Keras y muchos otros pueden acelerar su desarrollo de aprendizaje automático significativamente. TensorFlow 2.3 ha introducido mejoras significativas con respecto a su predecesor, algunas de las cuales encontrará e implementará aquí.\n",
    "\n",
    "Al final de esta tarea, usted será capaz de hacer lo siguiente en TensorFlow 2.3:\n",
    "\n",
    "* Utilizar `tf.Variable` para modificar el estado de una variable\n",
    "* Explicar la diferencia entre una variable y una constante\n",
    "* Entrenar una red neuronal en un conjunto de datos TensorFlow\n",
    "\n",
    "Los marcos de programación como TensorFlow no sólo reducen el tiempo de codificación, sino que también pueden realizar optimizaciones que aceleran el propio código. \n",
    "\n",
    "## Nota importante sobre el envío al AutoGrader\n",
    "\n",
    "Antes de enviar su tarea al AutoGrader, asegúrese de que no está haciendo lo siguiente:\n",
    "\n",
    "1. No ha añadido ninguna declaración _extra_ `print` en la tarea.\n",
    "2. No ha añadido ninguna celda de código _extra_ en la tarea.\n",
    "3. No ha cambiado ningún parámetro de la función.\n",
    "4. No ha utilizado ninguna variable global dentro de sus ejercicios calificados. A menos que se le indique específicamente que lo haga, por favor absténgase de hacerlo y utilice las variables locales en su lugar.\n",
    "5. No está cambiando el código de asignación donde no es necesario, como la creación de variables _extra_.\n",
    "\n",
    "Si hace algo de lo siguiente, obtendrá un error como `Grader no encontrado` (o similarmente inesperado) al enviar su tarea. Antes de pedir ayuda/depurar los errores de su tarea, compruebe esto primero. Si este es el caso, y no recuerda los cambios que ha realizado, puede obtener una nueva copia de la tarea siguiendo estas [instrucciones](https://www.coursera.org/learn/deep-neural-network/supplement/QWEnZ/h-ow-to-refresh-your-workspace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1- Packages](#1)\n",
    "    - [1.1 - Checking TensorFlow Version](#1-1)\n",
    "- [2 - Basic Optimization with GradientTape](#2)\n",
    "    - [2.1 - Linear Function](#2-1)\n",
    "        - [Exercise 1 - linear_function](#ex-1)\n",
    "    - [2.2 - Computing the Sigmoid](#2-2)\n",
    "        - [Exercise 2 - sigmoid](#ex-2)\n",
    "    - [2.3 - Using One Hot Encodings](#2-3)\n",
    "        - [Exercise 3 - one_hot_matrix](#ex-3)\n",
    "    - [2.4 - Initialize the Parameters](#2-4)\n",
    "        - [Exercise 4 - initialize_parameters](#ex-4)\n",
    "- [3 - Building Your First Neural Network in TensorFlow](#3)\n",
    "    - [3.1 - Implement Forward Propagation](#3-1)\n",
    "        - [Exercise 5 - forward_propagation](#ex-5)\n",
    "    - [3.2 Compute the Cost](#3-2)\n",
    "        - [Exercise 6 - compute_cost](#ex-6)\n",
    "    - [3.3 - Train the Model](#3-3)\n",
    "- [4 - Bibliography](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "from tensorflow.python.ops.resource_variable_ops import ResourceVariable\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - Comprobación de la versión de TensorFlow \n",
    "\n",
    "Utilizarás la versión 2.3 para esta tarea, para obtener la máxima velocidad y eficiencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Optimización básica con GradientTape\n",
    "\n",
    "La belleza de TensorFlow 2 está en su simplicidad. Básicamente, todo lo que necesitas hacer es implementar la propagación hacia adelante a través de un gráfico computacional. TensorFlow calculará las derivadas por ti, moviéndose hacia atrás a través del gráfico registrado con `GradientTape`. Lo único que te queda por hacer es especificar la función de coste y el optimizador que quieres utilizar. \n",
    "\n",
    "Al escribir un programa TensorFlow, el objeto principal que se utiliza y transforma es el `tf.Tensor`. Estos tensores son el equivalente en TensorFlow a los arrays de Numpy, es decir, arrays multidimensionales de un tipo de datos determinado que también contienen información sobre el gráfico computacional.\n",
    "\n",
    "A continuación, utilizarás `tf.Variable` para almacenar el estado de tus variables. Las variables sólo pueden crearse una vez, ya que su valor inicial define la forma y el tipo de la variable. Además, el arg `dtype` en `tf.Variable` puede ser establecido para permitir que los datos sean convertidos a ese tipo. Pero si no se especifica ninguno, se mantendrá el tipo de datos si el valor inicial es un tensor, o lo decidirá `convert_to_tensor`. Generalmente es mejor que se especifique directamente, ¡así no se rompe nada!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí llamarás al conjunto de datos TensorFlow creado en un archivo HDF5, que puedes utilizar en lugar de un array Numpy para almacenar tus conjuntos de datos. ¡Puedes pensar en esto como un generador de datos TensorFlow! \n",
    "\n",
    "Utilizarás el conjunto de datos Hand sign, que está compuesto por imágenes con forma 64x64x3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n",
    "test_dataset  = h5py.File('datasets/test_signs.h5', \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.data.Dataset.from_tensor_slices(train_dataset['train_set_x'])\n",
    "y_train = tf.data.Dataset.from_tensor_slices(train_dataset['train_set_y'])\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(test_dataset['test_set_x'])\n",
    "y_test = tf.data.Dataset.from_tensor_slices(test_dataset['test_set_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que los conjuntos de datos de TensorFlow son generadores, no puedes acceder directamente a su contenido a menos que iteres sobre ellos en un bucle for, o creando explícitamente un iterador de Python usando `iter` y consumiendo sus\n",
    "elementos con `next`. Además, puedes inspeccionar la \"forma\" y el \"tipo\" de cada elemento utilizando el atributo \"element_spec\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(x_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos que utilizarás durante esta tarea es un subconjunto de los dígitos del lenguaje de signos. Contiene seis clases diferentes que representan los dígitos del 0 al 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set()\n",
    "for element in y_train:\n",
    "    unique_labels.add(element.numpy())\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ver algunas de las imágenes del conjunto de datos ejecutando la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_iter = iter(x_train)\n",
    "labels_iter = iter(y_train)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    ax = plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(next(images_iter).numpy().astype(\"uint8\"))\n",
    "    plt.title(next(labels_iter).numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay una diferencia adicional entre los conjuntos de datos de TensorFlow y las matrices de Numpy: Si necesitas transformar uno, invocarías el método `map` para aplicar la función pasada como argumento a cada uno de los elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    \"\"\"\n",
    "    Transformar una imagen en un tensor de forma (64 * 64 * 3, )\n",
    "    y normalizar sus componentes.\n",
    "    \n",
    "    Argumentos\n",
    "    imagen - Tensor.\n",
    "    \n",
    "    Devuelve \n",
    "    resultado -- Tensor transformado \n",
    "    \"\"\"\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.reshape(image, [-1,])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = x_train.map(normalize)\n",
    "new_test = x_test.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(new_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 - Función lineal\n",
    "\n",
    "Comencemos este ejercicio de programación calculando la siguiente ecuación: $Y = WX + b$, donde $W$ y $X$ son matrices aleatorias y b es un vector aleatorio. \n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1 - linear_function\n",
    "\n",
    "Calcule $WX + b$ donde $W, X$ y $b$ se extraen de una distribución normal aleatoria. W tiene forma (4, 3), X es (3,1) y b es (4,1). A modo de ejemplo, así se define una constante X con la forma (3,1):\n",
    "```python\n",
    "X = tf.constant(np.random.randn(3,1), name = \"X\")\n",
    "\n",
    "```\n",
    "Ten en cuenta que la diferencia entre `tf.constant` y `tf.Variable` es que puedes modificar el estado de una `tf.Variable` pero no puedes cambiar el estado de una `tf.constant`.\n",
    "```\n",
    "\n",
    "Las siguientes funciones pueden resultarle útiles: \n",
    "- tf.matmul(..., ...) para hacer una multiplicación de matrices\n",
    "- tf.add(..., ...) para hacer una suma\n",
    "- np.random.randn(...) para inicializar aleatoriamente\n",
    "    \n",
    "<p hidden> \n",
    "    X = tf.constant(np.random.randn(3,1), name = \"X\")\n",
    "    W = tf.constant(np.random.randn(4,3), name = \"w\")\n",
    "    b = tf.constant(np.random.randn(4,1), name = \"b\")\n",
    "    Y = tf.add( tf.matmul ( W, X ), b) \n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "397d354ecaa1a28936096002cde11279",
     "grade": false,
     "grade_id": "cell-002e5736767021c0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_function\n",
    "\n",
    "def linear_function():\n",
    "    \"\"\"\n",
    "    Implements a linear function: \n",
    "            Initializes X to be a random tensor of shape (3,1)\n",
    "            Initializes W to be a random tensor of shape (4,3)\n",
    "            Initializes b to be a random tensor of shape (4,1)\n",
    "    Returns: \n",
    "    result -- Y = WX + b \n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    \"\"\"\n",
    "    Note, to ensure that the \"random\" numbers generated match the expected results,\n",
    "    please create the variables in the order given in the starting code below.\n",
    "    (Do not re-arrange the order).\n",
    "    Nota, para asegurar que los números \"aleatorios\" generados coinciden con los resultados esperados,\n",
    "    por favor, cree las variables en el orden que se indica en el código de inicio.\n",
    "    (No reordene el orden).\n",
    "    \"\"\"\n",
    "    # (approx. 4 lines)\n",
    "    # X = ...\n",
    "    # W = ...\n",
    "    # b = ...\n",
    "    # Y = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3526a7fd39649d2a6516031720e46748",
     "grade": true,
     "grade_id": "cell-b4318ea155f136ab",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "result = linear_function()\n",
    "print(result)\n",
    "\n",
    "assert type(result) == EagerTensor, \"Use the TensorFlow API\"\n",
    "assert np.allclose(result, [[-2.15657382], [ 2.95891446], [-1.08926781], [-0.84538042]]), \"Error\"\n",
    "print(\"\\033[92mAll test passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "```\n",
    "result = \n",
    "[[-2.15657382]\n",
    " [ 2.95891446]\n",
    " [-1.08926781]\n",
    " [-0.84538042]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - Cálculo de la sigmoidea \n",
    "Increíble. Acabas de implementar una función lineal. TensorFlow ofrece una variedad de funciones de redes neuronales de uso común como `tf.sigmoid` y `tf.softmax`.\n",
    "\n",
    "Para este ejercicio, calcula el sigmoide de z. \n",
    "\n",
    "En este ejercicio, usted: Convierte tu tensor en el tipo `float32` usando `tf.cast`, luego calcula el sigmoide usando `tf.keras.activations.sigmoid`. \n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - sigmoid\n",
    "\n",
    "Implementa la función sigmoidea que aparece a continuación. Debe utilizar lo siguiente: \n",
    "\n",
    "- `tf.cast(\"...\", tf.float32)`\n",
    "- `tf.keras.activations.sigmoid(\"...\")`\n",
    "\n",
    "<p hidden>\n",
    "z = tf.cast(z, tf.float32)\n",
    "a = tf.keras.activations.sigmoid(z)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00b8d3d2338bb2ed68c9e316e5de9581",
     "grade": false,
     "grade_id": "cell-038bb4b7e61dd070",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calcula la sigmoidea de z\n",
    "    \n",
    "    Argumentos:\n",
    "    z -- valor de entrada, escalar o vector\n",
    "    \n",
    "    Devuelve: \n",
    "    a -- (tf.float32) la sigmoide de z\n",
    "    \"\"\"\n",
    "    # tf.keras.activations.sigmoid requires float16, float32, float64, complex64, or complex128.\n",
    "    \n",
    "    # (approx. 2 lines)\n",
    "    # z = ...z = tf.cast(z, tf.float32)\n",
    "    # a = tf.keras.activations.sigmoid(z)\n",
    "    # a = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return a\n",
    "# z = tf.cast(z, tf.float32)\n",
    "#     a = tf.keras.activations.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad1c73949744ba2205a0ad0d6f395915",
     "grade": true,
     "grade_id": "cell-a04f348c3fdbc2f2",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "result = sigmoid(-1)\n",
    "print (\"type: \" + str(type(result)))\n",
    "print (\"dtype: \" + str(result.dtype))\n",
    "print (\"sigmoid(-1) = \" + str(result))\n",
    "print (\"sigmoid(0) = \" + str(sigmoid(0.0)))\n",
    "print (\"sigmoid(12) = \" + str(sigmoid(12)))\n",
    "\n",
    "def sigmoid_test(target):\n",
    "    result = target(0)\n",
    "    assert(type(result) == EagerTensor)\n",
    "    assert (result.dtype == tf.float32)\n",
    "    assert sigmoid(0) == 0.5, \"Error\"\n",
    "    assert sigmoid(-1) == 0.26894143, \"Error\"\n",
    "    assert sigmoid(12) == 0.9999939, \"Error\"\n",
    "\n",
    "    print(\"\\033[92mAll test passed\")\n",
    "\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "<table>\n",
    "<tr> \n",
    "<td>\n",
    "type\n",
    "</td>\n",
    "<td>\n",
    "class 'tensorflow.python.framework.ops.EagerTensor'\n",
    "</td>\n",
    "</tr><tr> \n",
    "<td>\n",
    "dtype\n",
    "</td>\n",
    "<td>\n",
    "\"dtype: 'float32'\n",
    "</td>\n",
    "</tr>\n",
    "<tr> \n",
    "<td>\n",
    "Sigmoid(-1)\n",
    "</td>\n",
    "<td>\n",
    "0.2689414\n",
    "</td>\n",
    "</tr>\n",
    "<tr> \n",
    "<td>\n",
    "Sigmoid(0)\n",
    "</td>\n",
    "<td>\n",
    "0.5\n",
    "</td>\n",
    "</tr>\n",
    "<tr> \n",
    "<td>\n",
    "Sigmoid(12)\n",
    "</td>\n",
    "<td>\n",
    "0.999994\n",
    "</td>\n",
    "</tr> \n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-3'></a>\n",
    "### 2.3 - Uso de codificaciones en caliente\n",
    "\n",
    "Muchas veces en el aprendizaje profundo tendrás un vector $Y$ con números que van de $0$ a $C-1$, donde $C$ es el número de clases. Si $C$ es, por ejemplo, 4, entonces podrías tener el siguiente vector y que tendrás que convertir así:\n",
    "\n",
    "\n",
    "<img src=\"images/onehot.png\" style=\"width:600px;height:150px;\">\n",
    "\n",
    "Esto se llama codificación \"one hot\", porque en la representación convertida, exactamente un elemento de cada columna es \"hot\" (lo que significa que se establece en 1). Para hacer esta conversión en numpy, puede que tengas que escribir unas cuantas líneas de código. En TensorFlow, puedes usar una línea de código: \n",
    "\n",
    "- [tf.one_hot(labels, depth, axis=0)](https://www.tensorflow.org/api_docs/python/tf/one_hot)\n",
    "\n",
    "`axis=0` indica que el nuevo eje (axis)  se crea en la dimensión 0\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - one_hot_matrix\n",
    "\n",
    "Implemente la función de abajo para tomar una etiqueta y el número total de clases $C$, y devuelva la codificación de un caliente en una matriz de columnas. ¡Utilice `tf.one_hot()` para hacer esto, y `tf.reshape()` para remodelar su tensor one hot! \n",
    "\n",
    "- `tf.reshape(tensor, shape)`\n",
    "<p hidden>tf.reshape(tf.one_hot(label, depth, axis=0),shape=(-1,))</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44bfa91af0e57ca117ebf3acce902a28",
     "grade": false,
     "grade_id": "cell-15d9db613d8007bb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_hot_matrix\n",
    "def one_hot_matrix(label, depth=6):\n",
    "    \"\"\"\n",
    "    Calcula la codificación en caliente de una sola etiqueta\n",
    "    \n",
    "    Argumentos:\n",
    "        label -- (int) Etiquetas categóricas\n",
    "        depth -- (int) Número de clases diferentes que puede tomar la etiqueta\n",
    "    \n",
    "    Devuelve:\n",
    "         one_hot -- tf.Tensor Una matriz de una columna con la codificación one hot.\n",
    "    \"\"\"\n",
    "    # (approx. 1 line)\n",
    "    # one_hot =...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f377cdff475cec1b37293b70e26b74a4",
     "grade": true,
     "grade_id": "cell-100c1b3328215913",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_matrix_test(target):\n",
    "    label = tf.constant(1)\n",
    "    depth = 4\n",
    "    result = target(label, depth)\n",
    "    print(\"Test 1:\",result)\n",
    "    assert result.shape[0] == depth, \"Use the parameter depth\"\n",
    "    assert np.allclose(result, [0., 1. ,0., 0.] ), \"Wrong output. Use tf.one_hot\"\n",
    "    label_2 = [2]\n",
    "    result = target(label_2, depth)\n",
    "    print(\"Test 2:\", result)\n",
    "    assert result.shape[0] == depth, \"Use the parameter depth\"\n",
    "    assert np.allclose(result, [0., 0. ,1., 0.] ), \"Wrong output. Use tf.reshape as instructed\"\n",
    "    \n",
    "    print(\"\\033[92mAll test passed\")\n",
    "\n",
    "one_hot_matrix_test(one_hot_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "Test 1: tf.Tensor([0. 1. 0. 0.], shape=(4,), dtype=float32)\n",
    "Test 2: tf.Tensor([0. 0. 1. 0.], shape=(4,), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_test = y_test.map(one_hot_matrix)\n",
    "new_y_train = y_train.map(one_hot_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(new_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-4'></a>\n",
    "### 2.4 - Inicializar los parámetros \n",
    "\n",
    "Ahora vas a inicializar un vector de números con el inicializador Glorot. La función a la que llamarás es `tf.keras.initializers.GlorotNormal`, que extrae muestras de una distribución normal truncada centrada en 0, con `stddev = sqrt(2 / (fan_in + fan_out))`, donde `fan_in` es el número de unidades de entrada y `fan_out` es el número de unidades de salida, ambas en el tensor de pesos. \n",
    "\n",
    "Para inicializar con ceros o unos puedes usar `tf.zeros()` o `tf.ones()` en su lugar. \n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - initialize_parameters\n",
    "\n",
    "Implementa la función de abajo para tomar una forma y devolver un array de números usando el inicializador GlorotNormal. \n",
    "\n",
    "\n",
    " - `tf.keras.initializers.GlorotNormal(seed=1)`\n",
    " - `tf.Variable(initializer(shape=())`\n",
    " \n",
    "<P hidden>\n",
    "    W1 = tf.Variable(initializer(shape=(25, 12288)))\n",
    "    b1 = tf.Variable(initializer(shape=(25, 1)))\n",
    "    W2 = tf.Variable(initializer(shape=(12, 25)))\n",
    "    b2 = tf.Variable(initializer(shape=(12, 1)))\n",
    "    W3 = tf.Variable(initializer(shape=(6, 12)))\n",
    "    b3 = tf.Variable(initializer(shape=(6, 1)))\n",
    "<\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da48416c74797c83152e1080b08afb9d",
     "grade": false,
     "grade_id": "cell-1d5716c48a16debf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with TensorFlow. The shapes are:\n",
    "                        W1 : [25, 12288]\n",
    "                        b1 : [25, 1]\n",
    "                        W2 : [12, 25]\n",
    "                        b2 : [12, 1]\n",
    "                        W3 : [6, 12]\n",
    "                        b3 : [6, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "                                \n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed=1)   \n",
    "    #(approx. 6 lines of code)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd3fe0b5ed777771156c071d9373e47a",
     "grade": true,
     "grade_id": "cell-11012e1fada40919",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_test(target):\n",
    "    parameters = target()\n",
    "\n",
    "    values = {\"W1\": (25, 12288),\n",
    "              \"b1\": (25, 1),\n",
    "              \"W2\": (12, 25),\n",
    "              \"b2\": (12, 1),\n",
    "              \"W3\": (6, 12),\n",
    "              \"b3\": (6, 1)}\n",
    "\n",
    "    for key in parameters:\n",
    "        print(f\"{key} shape: {tuple(parameters[key].shape)}\")\n",
    "        assert type(parameters[key]) == ResourceVariable, \"All parameter must be created using tf.Variable\"\n",
    "        assert tuple(parameters[key].shape) == values[key], f\"{key}: wrong shape\"\n",
    "        assert np.abs(np.mean(parameters[key].numpy())) < 0.5,  f\"{key}: Use the GlorotNormal initializer\"\n",
    "        assert np.std(parameters[key].numpy()) > 0 and np.std(parameters[key].numpy()) < 1, f\"{key}: Use the GlorotNormal initializer\"\n",
    "\n",
    "    print(\"\\033[92mAll test passed\")\n",
    "    \n",
    "initialize_parameters_test(initialize_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "W1 shape: (25, 12288)\n",
    "b1 shape: (25, 1)\n",
    "W2 shape: (12, 25)\n",
    "b2 shape: (12, 1)\n",
    "W3 shape: (6, 12)\n",
    "b3 shape: (6, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Building Your First Neural Network in TensorFlow\n",
    "\n",
    "En esta parte de la tarea construirás una red neuronal utilizando TensorFlow. Recuerda que hay dos partes para implementar un modelo TensorFlow:\n",
    "\n",
    "- Implementar la propagación hacia adelante\n",
    "- Recuperar los gradientes y entrenar el modelo\n",
    "\n",
    "¡Vamos a ponernos manos a la obra!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - Implement Forward Propagation \n",
    "\n",
    "Uno de los grandes puntos fuertes de TensorFlow radica en el hecho de que sólo tienes que implementar la función de propagación hacia adelante y él llevará la cuenta de las operaciones que hiciste para calcular la propagación hacia atrás automáticamente.  \n",
    "\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - forward_propagation\n",
    "\n",
    "Implementar la función `forward_propagation`.\n",
    "\n",
    "**Nota** Utilizar sólo la API TF. \n",
    "\n",
    "- tf.math.add\n",
    "- tf.linalg.matmul\n",
    "- tf.keras.activations.relu\n",
    "\n",
    "<p hidden>\n",
    "Z1 = tf.math.add(tf.linalg.matmul(W1,X),b1)    \n",
    "A1 = tf.keras.activations.relu(Z1)          \n",
    "Z2 = tf.math.add(tf.linalg.matmul(W2,A1),b2)\n",
    "A2 = tf.keras.activations.relu(Z2)          \n",
    "Z3 = tf.math.add(tf.linalg.matmul(W3,A2),b3)\n",
    "<\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e52024ce85f80538e02ad44ff9a6e334",
     "grade": false,
     "grade_id": "cell-23b6d82b3443e298",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia adelante para el modelo LINEAL -> RELU -> LINEAL -> RELU -> LINEAL\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- marcador de entrada del conjunto de datos, de forma (tamaño de entrada, número de ejemplos)\n",
    "    parameters -- diccionario python que contiene sus parámetros \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  las formas se dan en initialize_parameters\n",
    "\n",
    "    Devuelve:\n",
    "    Z3 -- la salida de la última unidad LINEAL\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    #(approx. 5 lines)                   # Numpy Equivalents:\n",
    "    # Z1 = ...                           # Z1 = np.dot(W1, X) + b1\n",
    "    # A1 = ...                           # A1 = relu(Z1)\n",
    "    # Z2 = ...                           # Z2 = np.dot(W2, A1) + b2\n",
    "    # A2 = ...                           # A2 = relu(Z2)\n",
    "    # Z3 = ...                           # Z3 = np.dot(W3, A2) + b3\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adf252febb4ae71156322e225433ae83",
     "grade": true,
     "grade_id": "cell-728b002a6a88ceb1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation_test(target, examples):\n",
    "    minibatches = examples.batch(2)\n",
    "    parametersk = initialize_parameters()\n",
    "    W1 = parametersk['W1']\n",
    "    b1 = parametersk['b1']\n",
    "    W2 = parametersk['W2']\n",
    "    b2 = parametersk['b2']\n",
    "    W3 = parametersk['W3']\n",
    "    b3 = parametersk['b3']\n",
    "    index = 0\n",
    "    minibatch = list(minibatches)[0]\n",
    "    with tf.GradientTape() as tape:\n",
    "        forward_pass = target(tf.transpose(minibatch), parametersk)\n",
    "        print(forward_pass)\n",
    "        fake_cost = tf.reduce_mean(forward_pass - np.ones((6,2)))\n",
    "\n",
    "        assert type(forward_pass) == EagerTensor, \"Your output is not a tensor\"\n",
    "        assert forward_pass.shape == (6, 2), \"Last layer must use W3 and b3\"\n",
    "        assert np.allclose(forward_pass, \n",
    "                           [[-0.13430887,  0.14086473],\n",
    "                            [ 0.21588647, -0.02582335],\n",
    "                            [ 0.7059658,   0.6484556 ],\n",
    "                            [-1.1260961,  -0.9329492 ],\n",
    "                            [-0.20181894, -0.3382722 ],\n",
    "                            [ 0.9558965,   0.94167566]]), \"Output does not match\"\n",
    "    index = index + 1\n",
    "    trainable_variables = [W1, b1, W2, b2, W3, b3]\n",
    "    grads = tape.gradient(fake_cost, trainable_variables)\n",
    "    assert not(None in grads), \"Wrong gradients. It could be due to the use of tf.Variable whithin forward_propagation\"\n",
    "    print(\"\\033[92mAll test passed\")\n",
    "\n",
    "forward_propagation_test(forward_propagation, new_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "tf.Tensor(\n",
    "[[-0.13430887  0.14086473]\n",
    " [ 0.21588647 -0.02582335]\n",
    " [ 0.7059658   0.6484556 ]\n",
    " [-1.1260961  -0.9329492 ]\n",
    " [-0.20181894 -0.3382722 ]\n",
    " [ 0.9558965   0.94167566]], shape=(6, 2), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "### 3.2 Compute the Cost\n",
    "\n",
    "ATodo lo que tiene que hacer ahora es definir la función de pérdida que va a utilizar. Para este caso, como tenemos un problema de clasificación con 6 etiquetas, ¡una entropía cruzada categórica funcionará! \n",
    "\n",
    "<a name='ex-6'></a>\n",
    "### Exercise 6 -  compute_cost\n",
    "\n",
    "Implementa la función de coste que se muestra a continuación. \n",
    "- Es importante tener en cuenta que las entradas \"`y_pred`\" y \"`y_true`\" de [tf.keras.losses.categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy) se espera que sean de la forma (número de ejemplos, num_classes). \n",
    "\n",
    "- `tf.reduce_mean` básicamente hace la suma sobre los ejemplos.\n",
    "<p hidden>\n",
    "cost = tf.reduce_mean( tf.keras.losses.categorical_crossentropy(\n",
    "        tf.transpose(labels),\n",
    "        tf.transpose(logits),from_logits=True))\n",
    "<\\p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec62b123a471aa12f20842442c73ec68",
     "grade": false,
     "grade_id": "cell-e6cc4d7fefeed231",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(logits, labels):\n",
    "    \"\"\"\n",
    "    Calcula el coste\n",
    "    \n",
    "    Argumentos:\n",
    "    logits -- salida de la propagación hacia delante (salida de la última unidad LINEAL), de forma (6, num_examples)\n",
    "    labels -- vector de etiquetas \"verdaderas\", de la misma forma que Z3\n",
    "    \n",
    "    Devuelve:\n",
    "    cost - Tensor de la función de coste\n",
    "    \"\"\"\n",
    "    \n",
    "    #(1 line of code)\n",
    "    # cost = ...\n",
    "    # YOUR CODE STARTS HERE  \n",
    "\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d42866ad09b737ac5f667013b78efcfc",
     "grade": true,
     "grade_id": "cell-9bf72affa2e7b1b5",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_cost_test(target, Y):\n",
    "    pred = tf.constant([[ 2.4048107,   5.0334096 ],\n",
    "             [-0.7921977,  -4.1523376 ],\n",
    "             [ 0.9447198,  -0.46802214],\n",
    "             [ 1.158121,    3.9810789 ],\n",
    "             [ 4.768706,    2.3220146 ],\n",
    "             [ 6.1481323,   3.909829  ]])\n",
    "    minibatches = Y.batch(2)\n",
    "    for minibatch in minibatches:\n",
    "        result = target(pred, tf.transpose(minibatch))\n",
    "        break\n",
    "        \n",
    "    print(result)\n",
    "    assert(type(result) == EagerTensor), \"Use the TensorFlow API\"\n",
    "    assert (np.abs(result - (0.25361037 + 0.5566767) / 2.0) < 1e-7), \"Test does not match. Did you get the mean of your cost functions?\"\n",
    "\n",
    "    print(\"\\033[92mAll test passed\")\n",
    "\n",
    "compute_cost_test(compute_cost, new_y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```\n",
    "tf.Tensor(0.4051435, shape=(), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-3'></a>\n",
    "### 3.3 - Entrenar el modelo\n",
    "\n",
    "Hablemos de los optimizadores. Especificarás el tipo de optimizador en una línea, en este caso `tf.keras.optimizers.Adam` (aunque puedes usar otros como SGD), y luego lo llamarás dentro del bucle de entrenamiento. \n",
    "\n",
    "Fíjate en la función `tape.gradient`: permite recuperar las operaciones registradas para la diferenciación automática dentro del bloque `GradientTape`. Luego, al llamar al método del optimizador `apply_gradients`, se aplicarán las reglas de actualización del optimizador a cada parámetro entrenable. Al final de esta tarea, encontrarás una documentación que explica esto con más detalle, pero por ahora, una simple explicación será suficiente. ;) \n",
    "\n",
    "\n",
    "Aquí deberías tomar nota de un importante paso extra que se ha añadido al proceso de entrenamiento por lotes: \n",
    "\n",
    "- `tf.Data.dataset = dataset.prefetch(8)` \n",
    "\n",
    "Lo que esto hace es prevenir un cuello de botella en la memoria que puede ocurrir cuando se lee del disco. La función `prefetch()` aparta algunos datos y los mantiene listos para cuando se necesiten. Lo hace creando un conjunto de datos fuente a partir de sus datos de entrada, aplicando una transformación para preprocesar los datos, y luego iterando sobre el conjunto de datos el número especificado de elementos a la vez. Esto funciona porque la iteración es en flujo, por lo que los datos no necesitan caber en la memoria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implementa una red neuronal tensorflow de tres capas: LINEAR->RELU->LINEAL->RELU->LINEAL->SOFTMAX.\n",
    "    \n",
    "    Argumentos:\n",
    "    X_train -- conjunto de entrenamiento, de forma (tamaño de entrada = 12288, número de ejemplos de entrenamiento = 1080)\n",
    "    Y_train -- conjunto de prueba, de forma (tamaño de salida = 6, número de ejemplos de entrenamiento = 1080)\n",
    "    X_test -- conjunto de entrenamiento, de forma (tamaño de entrada = 12288, número de ejemplos de entrenamiento = 120)\n",
    "    Y_test -- conjunto de prueba, de forma (tamaño de salida = 6, número de ejemplos de prueba = 120)\n",
    "    learning_rate -- tasa de aprendizaje de la optimización\n",
    "    num_epochs -- número de épocas del bucle de optimización\n",
    "    minibatch_size -- tamaño de un minibatch\n",
    "    print_cost -- Verdadero para imprimir el coste cada 10 épocas\n",
    "    \n",
    "    Devuelve:\n",
    "    parameters -- parámetros aprendidos por el modelo. Pueden ser utilizados para predecir.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    \n",
    "    # Initialize your parameters\n",
    "    #(1 line)\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    \n",
    "    # El CategoricalAccuracy registrará la precisión de este problema multiclase\n",
    "    test_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    dataset = tf.data.Dataset.zip((X_train, Y_train))\n",
    "    test_dataset = tf.data.Dataset.zip((X_test, Y_test))\n",
    "    \n",
    "    # Podemos obtener el número de elementos de un conjunto de datos utilizando el método de cardinalidad\n",
    "    m = dataset.cardinality().numpy()\n",
    "    \n",
    "    minibatches = dataset.batch(minibatch_size).prefetch(8)\n",
    "    test_minibatches = test_dataset.batch(minibatch_size).prefetch(8)\n",
    "    #X_train = X_train.batch(minibatch_size, drop_remainder=True).prefetch(8)# <<< extra step    \n",
    "    #Y_train = Y_train.batch(minibatch_size, drop_remainder=True).prefetch(8) # loads memory faster \n",
    "\n",
    "    # Do the training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_cost = 0.\n",
    "        \n",
    "        #necesitamos reiniciar el objeto para empezar a medir desde 0 la precisión en cada época\n",
    "        train_accuracy.reset_states()\n",
    "        \n",
    "        for (minibatch_X, minibatch_Y) in minibatches:\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # 1. predict\n",
    "                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)\n",
    "\n",
    "                # 2. loss\n",
    "                minibatch_cost = compute_cost(Z3, tf.transpose(minibatch_Y))\n",
    "\n",
    "            # Acumulamos la precisión de todos los lotes\n",
    "            train_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\n",
    "            \n",
    "            trainable_variables = [W1, b1, W2, b2, W3, b3]\n",
    "            grads = tape.gradient(minibatch_cost, trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "            epoch_cost += minibatch_cost\n",
    "        \n",
    "        # Dividimos el coste de la época entre el número de muestras\n",
    "        epoch_cost /= m\n",
    "\n",
    "        # Print the cost every 10 epochs\n",
    "        if print_cost == True and epoch % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            print(\"Train accuracy:\", train_accuracy.result())\n",
    "            \n",
    "            # Evaluamos el conjunto de pruebas cada 10 épocas para evitar la sobrecarga computacional\n",
    "            for (minibatch_X, minibatch_Y) in test_minibatches:\n",
    "                Z3 = forward_propagation(tf.transpose(minibatch_X), parameters)\n",
    "                test_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\n",
    "            print(\"Test_accuracy:\", test_accuracy.result())\n",
    "\n",
    "            costs.append(epoch_cost)\n",
    "            train_acc.append(train_accuracy.result())\n",
    "            test_acc.append(test_accuracy.result())\n",
    "            test_accuracy.reset_states()\n",
    "\n",
    "\n",
    "    return parameters, costs, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs, train_acc, test_acc = model(new_train, new_y_train, new_test, new_y_test, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "```\n",
    "Cost after epoch 0: 0.057612\n",
    "Train accuracy: tf.Tensor(0.17314816, shape=(), dtype=float32)\n",
    "Test_accuracy: tf.Tensor(0.24166666, shape=(), dtype=float32)\n",
    "Cost after epoch 10: 0.049332\n",
    "Train accuracy: tf.Tensor(0.35833332, shape=(), dtype=float32)\n",
    "Test_accuracy: tf.Tensor(0.3, shape=(), dtype=float32)\n",
    "...\n",
    "```\n",
    "Numbers you get can be different, just check that your loss is going down and your accuracy going up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cost\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per fives)')\n",
    "plt.title(\"Learning rate =\" + str(0.0001))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train accuracy\n",
    "plt.plot(np.squeeze(train_acc))\n",
    "plt.ylabel('Train Accuracy')\n",
    "plt.xlabel('iterations (per fives)')\n",
    "plt.title(\"Learning rate =\" + str(0.0001))\n",
    "# Plot the test accuracy\n",
    "plt.plot(np.squeeze(test_acc))\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xlabel('iterations (per fives)')\n",
    "plt.title(\"Learning rate =\" + str(0.0001))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Felicitaciones.** Has llegado al final de esta tarea, y al final del material de esta semana. ¡Un trabajo increíble construyendo una red neuronal en TensorFlow 2.3! \n",
    "\n",
    "Aquí hay un resumen rápido de todo lo que acabas de lograr:\n",
    "\n",
    "- Usaste `tf.Variable` para modificar tus variables\n",
    "- Entrenado una red neuronal en un conjunto de datos de TensorFlow\n",
    "\n",
    "Ahora eres capaz de aprovechar el poder de TensorFlow para crear cosas interesantes, más rápido. Muy bien. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Bibliography \n",
    "\n",
    "In this assignment, you were introducted to `tf.GradientTape`, which records operations for differentation. Here are a couple of resources for diving deeper into what it does and why: \n",
    "\n",
    "Introduction to Gradients and Automatic Differentiation: \n",
    "https://www.tensorflow.org/guide/autodiff \n",
    "\n",
    "GradientTape documentation:\n",
    "https://www.tensorflow.org/api_docs/python/tf/GradientTape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
